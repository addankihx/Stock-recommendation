{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions for all tickers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes how the training and testing of all tickers was handled. A large memory is required to execute the code. A local execution of the code was therefore not possible. Amazon Web Services (AWS) was used to execute the code. Since AWS cannot execute the code on notebooks, it was developed in Visual Studio Code in \".py\" format. The code is stored under the name \"2.1.3.2 Prediction for all tickers.py\". \n",
    "\n",
    "The code described in this notebook was executed using different sklearn methods, different features and different time courses. All used settings and their results are listed under the chapter \"results\". In the example below, the Support Vector Machine is used with the following setting: \n",
    "- all features (Open, Close, Low, High and Volume)\n",
    "- Training and testing with 50 tickers \n",
    "- time courses of 10, 20, 30, 40, 50, 60, 70, 80 and 90 days\n",
    "- Grid Search was used to determine the ideal values for the SVM. These are included: C = 100, gamma = 0.001 and kernel = rbf \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content\n",
    " 1. Structure of the code\n",
    " 2. Explanation of function prepareTrainingData()\n",
    " 3. Explanation of function train(x, y)\n",
    " 4. Explanation of function classify_2018(classifier)\n",
    " 5. Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Structure of the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code contains a main function in which several functions are called. The function \"prepareTrainingData()\" prepares the data for training and testing. For this purpose, all ticker data is read in and merged with the recommendations for action (the training labels). In the function train(x, y) a sklearn method is applied to the test and training result. The function classify_2018(classifier) finally predicts the recommended actions for the trading days in 2018. At the end of the main function the results are saved in a csv-file. The code for the main function is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    start_time = time.time()\n",
    "    x,y = prepareTrainingData()\n",
    "    classifier = train(x, y)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time -  start_time\n",
    "\n",
    "    result_str_lst = classify_2018(classifier)\n",
    "\n",
    "    # Transfer list to DataFrame and save\n",
    "    kaggle = pd.DataFrame(data=result_str_lst, columns=['Id', 'Category'])\n",
    "    kaggle = kaggle.to_csv('Predictions/SVM_Grid_Search_50_tickers.csv', index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is very flexible so that the time courses, used features and the number of tickers used for training can be adjusted easily. In the following, time histories of 10 times 10 days are used and all available features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_time_range = [i*10 for i in range(10)]\n",
    "features_used = ['Open', 'Close', 'Low', 'High', 'Volume']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next chapters the functions \n",
    "- prepareTrainingData()\n",
    "- train(x, y)\n",
    "- classify_2018(classifier)\n",
    "\n",
    "are explained in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Explanation of function prepareTrainingData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"prepareTrainingData()\" contains the function \"getAllTickerTimeseries_GT_Df_serial(ticker_file_names, df_train_label)\", which in turn calls functions. After a function has been described, the function it uses are described in more detail. These are marked in bold.\n",
    "\n",
    "With **\"getAllTickerTimeseries_GT_Df_serial\"** (GT: Ground Truth) the prepared data is loaded. Lines with \"nan\" or \"inf\" are removed because sklearn methods cannot handle these values. The created dataframe \"all_stocks_df\" is divided into x and y. Y contains the training labels (recommended actions for the trading day). X contains the feature values except the date.\n",
    "\n",
    "The function \"prepareTrainingData()\" looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareTrainingData():\n",
    "    df_train_label = pd.read_csv(join(dir_path, 'labels_train.csv'), header=0, index_col=0)\n",
    "\n",
    "    all_stocks_df = getAllTickerTimeseries_GT_Df_serial(ticker_file_names, df_train_label)\n",
    "    all_stocks_df = all_stocks_df.dropna(axis = 0)\n",
    "    all_stocks_df.replace([np.inf, -np.inf], np.nan).dropna(axis=0, inplace=True)\n",
    "\n",
    "    cols_x = list(all_stocks_df)\n",
    "    cols_x.remove(\"Date\")\n",
    "    cols_x.remove(\"Y\")\n",
    "\n",
    "    x = all_stocks_df[cols_x]\n",
    "    y = all_stocks_df[\"Y\"]\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the function \"getAllTickerTimeseries_GT_Df_serial(ticker_file_names, df_train_label)\" the dataframe \"all_stocks_df\" is built. \n",
    "A certain number of tickers, in this example 50, are read in one after the other. The formatted data is contained in function **\"getTickerTimeseries_GT_Df(file_name, df_train_label)\"**, which is assigned to \"stock_complete_df\" below. When the data for \"stock_complete_df\" is complete, it is added to \"all_stocks_df\". If the data is not complete, \"not complete\" is output. In order for a stock to be \"complete\", stock data must be available for all trading days in 2018 and for the last 90 trading days in 2017.\n",
    "\n",
    "With the function \"getAllTickerTimeseries_GT_Df_multiprocess(ticker_file_names, df_train_label)\", loading has been parallelized to achieve faster loading times. However, the parallelization did not result in a significant improvement of the loading time, which is why serial loading is still used with the \"getAllTickerTimeseries_GT_Df_serial(ticker_file_names, df_train_label)\" function. A description of parallel loading is therefore not given here. \n",
    "\n",
    "The function \"getAllTickerTimeseries_GT_Df_serial(ticker_file_names, df_train_label)\" is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllTickerTimeseries_GT_Df_serial(ticker_file_names, df_train_label):\n",
    "    all_stocks_df = DataFrame()\n",
    "\n",
    "    first_stock_b = True\n",
    "    file_ctr = 0\n",
    "\n",
    "    num_samples = 50\n",
    "    ticker_file_names = [ticker_file_names[i] for i in np.random.randint(0, len(ticker_file_names), num_samples)]\n",
    "\n",
    "    for file_name in ticker_file_names:\n",
    "        ticker_name = file_name.replace(\".csv\", \"\")\n",
    "        file_ctr += 1\n",
    "        stock_complete_df = getTickerTimeseries_GT_Df(file_name, df_train_label)\n",
    "        \n",
    "        if stock_complete_df is not None:\n",
    "            if first_stock_b:\n",
    "                all_stocks_df = stock_complete_df\n",
    "                first_stock_b = False\n",
    "            else:\n",
    "                all_stocks_df = all_stocks_df.append(stock_complete_df, ignore_index=True)\n",
    "                all_stocks_df = all_stocks_df.sort_values('Date')\n",
    "        else:\n",
    "            print(ticker_name + \" not complete\")\n",
    "    return all_stocks_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In \"getTickerTimeseries_GT_Df(file_name, df_train_label)\" the selected tickers are loaded first. The completeness of the data is checked with **\"hasTickerAllDates(df_ticker)\"** (which, as described above, checks the availibility of all necessary stock data in 2017 and 2018). When the ticker data is complete, timeseries are mapped with function **\"getTickerTimeseriesDf(df_ticker)\"**. Lines with \"nan\" or \"inf\" are removed because sklearn methods cannot handle these values. The training label (recommended action) is then included in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTickerTimeseries_GT_Df(file_name, df_train_label):\n",
    "    ticker_name = file_name.replace(\".csv\", \"\")\n",
    "\n",
    "    print(\"Loading: \" + file_name)\n",
    "\n",
    "    # Load data frame\n",
    "    file_str = join(ticker_path, file_name)\n",
    "    df_ticker = pd.read_csv(file_str)\n",
    "    df_ticker.columns = ['Date', 'Open', 'Close', 'Low', 'High', 'Volume']\n",
    "\n",
    "    # if ticker is not valid, then write all zeros in the output file\n",
    "    ticker_data_valid = hasTickerAllDates(df_ticker)\n",
    "    if not ticker_data_valid:\n",
    "        return None\n",
    "\n",
    "    stock_merged_df = getTickerTimeseriesDf(df_ticker)\n",
    "    stock_merged_df = stock_merged_df.dropna(axis = 0)\n",
    "    stock_merged_df.replace([np.inf, -np.inf], np.nan).dropna(axis=0, inplace=True)\n",
    "\n",
    "    # Add dependend variable\n",
    "    labels_df = df_train_label.loc[:, df_train_label.columns.intersection([ticker_name])]\n",
    "    labels_df = labels_df.rename(index=str, columns={ticker_name: 'Y'})\n",
    "    \n",
    "    stock_complete_df = pd.merge(stock_merged_df, labels_df[['Y']], on='Date')\n",
    "    stock_complete_df = stock_complete_df.sort_values('Date')\n",
    "\n",
    "    return stock_complete_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The check whether the required dates are given is carried out on the basis of the list \"all_boersen_days\", in which all trading days are listed:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasTickerAllDates(ticker_df):\n",
    "    res = True\n",
    "    for date_str in all_boersen_days:\n",
    "        if not date_str in ticker_df['Date'].values:\n",
    "            res = False\n",
    "            continue\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following function \"getTickerTimeseriesDf(df_ticker)\" ticker data is merged with the timeseries. Each feature is read in one after the other with the function **\"getFeatureTimeseriesDf(feature, feature_df)\"** and prepared accordingly. The preparation consists of normalizing values with the function **\"normalizeFeatureDf(feature_df)\"** and removing nan and inf values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTickerTimeseriesDf(df_ticker):\n",
    "    stock_merged_df = DataFrame()\n",
    "    first_feature_b = True\n",
    "\n",
    "    for feature in features_used:\n",
    "        feature_df = df_ticker.copy(deep=True)\n",
    "        # drop everything except of the current feature and the date\n",
    "        feature_df = feature_df[[\"Date\", feature]]\n",
    "        feature_df = getFeatureTimeseriesDf(feature, feature_df)\n",
    "        feature_df = normalizeFeatureDf(feature_df)\n",
    "        feature_df = feature_df.dropna(axis = 0)\n",
    "        feature_df.replace([np.inf, -np.inf], np.nan).dropna(axis=0, inplace=True)\n",
    "\n",
    "        # merge current dataframe into overall dataframe\n",
    "        if first_feature_b:\n",
    "            stock_merged_df = feature_df\n",
    "            first_feature_b = False\n",
    "        else:\n",
    "            stock_merged_df = pd.merge(stock_merged_df, feature_df, on='Date')\n",
    "    return stock_merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"getFeatureTimeseriesDf(feature, feature_df)\"** receives a feature name (e.g. \"Open\", \"Volume\", ...) and a dataframe, which only contains a \"Date\" column and one feature column. It then builds a \"timeseries\" dataframe. That means, the columns are the timeseries of the feature (e.g. [t0 - t-10days], [t0 - t-20days], ...) and the rows are the samples (in this case the dates), that are later being used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatureTimeseriesDf(feature, feature_df):\n",
    "    timeseries_feature_df = series_to_supervised(feature, feature_df[feature].values, features_time_range)\n",
    "\n",
    "    feature_ = feature + \"_\"\n",
    "\n",
    "    for t in features_time_range:\n",
    "        if not t == features_time_range[0]:\n",
    "            f_0 = timeseries_feature_df[feature_ + str(features_time_range[0])]\n",
    "            f_t = timeseries_feature_df[feature_ + str(t)]\n",
    "            f_t = f_t - f_0\n",
    "            feature_df[feature_ + str(t)] = f_t\n",
    "    if len(features_time_range) > 1:\n",
    "        feature_df = feature_df.drop([feature], axis=1)\n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"series_to_supervised(feature_name, data, time_range=[0], dropnan=True)\" function makes use od the Pandas dataframe.shift() function to build the timeseries dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(feature_name, data, time_range=[0], dropnan=True):\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in time_range:\n",
    "        cols.append(df.shift(-i))\n",
    "        names += [feature_name + \"_\" + str(i)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"MinMaxScaler\" and \"fit_transform\" is used for normalization, where the normalized values should be between -1 and 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeFeatureDf(feature_df):\n",
    "    tmp_df = feature_df.copy(deep=True)\n",
    "    tmp_df = tmp_df.drop([\"Date\"], axis=1)\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1), copy=False)\n",
    "    scaler.fit_transform(tmp_df.values)\n",
    "    tmp_df[\"Date\"] = feature_df[\"Date\"].values\n",
    "    feature_df = tmp_df\n",
    "\n",
    "    # move the \"Date\" column to the front of the dataframe\n",
    "    cols = list(feature_df)\n",
    "    cols.insert(0, cols.pop(cols.index('Date')))\n",
    "    feature_df = feature_df.loc[:, cols]\n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Explanation of function train(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to train sklearn classifier. The generated data from function prepareTrainingData() is used for this purpose. In the following the training and testing for the \"Support Vector Machine\" is shown: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, y):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.7, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Train Classifier\n",
    "    classifier = svm.SVC(kernel= 'rbf', gamma=0.001, C=100.0, max_iter=-1)\n",
    "    classifier.fit(x_train, y_train)\n",
    "    rf_score = classifier.score(x_test, y_test)\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimaze the parameter of SVM Grid Search was used for 50 tickers. Grid Search was used to determine the best settings for kernel, gamma and C. The result was:\n",
    "- C = 100\n",
    "- gamma = 0.001 \n",
    "- kernel = rbf\n",
    "\n",
    "The code for this is shown below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridSearch(x, y):\n",
    "    tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "    scores = ['precision', 'recall']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.5, test_size=0.5, random_state=42)\n",
    "\n",
    "    for score in scores:\n",
    "        print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "        print()\n",
    "\n",
    "        clf = GridSearchCV(svm.SVC(), tuned_parameters, cv=5, n_jobs=-1, scoring='%s_macro' % score)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        print(\"Best parameters set found on development set:\")\n",
    "        print()\n",
    "        print(clf.best_params_)\n",
    "        print()\n",
    "        print(\"Grid scores on development set:\")\n",
    "        print()\n",
    "        means = clf.cv_results_['mean_test_score']\n",
    "        stds = clf.cv_results_['std_test_score']\n",
    "        for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "            print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "                % (mean, std * 2, params))\n",
    "        print()\n",
    "\n",
    "        print(\"Detailed classification report:\")\n",
    "        print()\n",
    "        print(\"The model is trained on the full development set.\")\n",
    "        print(\"The scores are computed on the full evaluation set.\")\n",
    "        print()\n",
    "        y_true, y_pred = y_test, clf.predict(X_test)\n",
    "        print(classification_report(y_true, y_pred))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Explanation of function classify_2018(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function \"classify_2018(classifier)\" is used to predict the recommended actions for 2018. The function receives the classifier, that has been returned by train(x,y) (in this example the SVM).\n",
    "\n",
    "For each ticker, the data is loaded first and then the tickers are checked for completeness with the function \"hasTickerAllDates(df_ticker)\", which has already been described in chapter 2. If the data for a ticker is not completely available, \"0\" is being predicted for each trading day in 2018. The dates to be predicted are derived from the function \"getTickerTimeseriesDf\" already described in Chapter 2. When all the data is complete, a recommendation for action is predicted for each trading day. The code for the function is shown below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_2018(classifier):\n",
    "    result_str_lst = list()\n",
    "\n",
    "    for file_name in ticker_file_names:\n",
    "        ticker_name = file_name.replace(\".csv\", \"\")\n",
    "\n",
    "        # 1 Load data frame\n",
    "        file_str = join(ticker_path, file_name)\n",
    "        df_ticker = pd.read_csv(file_str)\n",
    "        df_ticker.columns = ['Date', 'Open', 'Close', 'Low', 'High', 'Volume']\n",
    "\n",
    "        ticker_data_valid = hasTickerAllDates(df_ticker)\n",
    "\n",
    "        # if ticker is not valid, then write all zeros in the output file\n",
    "        if not ticker_data_valid:\n",
    "            for date_str in boersen_days_2018:\n",
    "                result_str_lst.append([date_str + \":\" + ticker_name, 0])\n",
    "            print(ticker_name + \" defaults to 0\")\n",
    "            continue\n",
    "\n",
    "        ticker_ts_df = getTickerTimeseriesDf(df_ticker)\n",
    "        ticker_ts_df = ticker_ts_df.dropna(axis = 0)\n",
    "        ticker_ts_df.replace([np.inf, -np.inf], np.nan).dropna(axis=0, inplace=True)\n",
    "\n",
    "        selected_dates_df = DataFrame()\n",
    "        selected_dates = list()\n",
    "        default_dates = list()\n",
    "\n",
    "        for date in boersen_days_2018:\n",
    "            x = ticker_ts_df.loc[ticker_ts_df['Date'] == date]\n",
    "            if not x.empty:\n",
    "                selected_dates_df = selected_dates_df.append(x)\n",
    "                selected_dates.append(date)\n",
    "            else:\n",
    "                default_dates.append(date)\n",
    "\n",
    "        selected_dates_df = selected_dates_df.drop(['Date'], axis=1)\n",
    "\n",
    "        prediction = classifier.predict(selected_dates_df)\n",
    "\n",
    "        for _ in range(len(prediction) + len(default_dates)):\n",
    "            if len(selected_dates) > 0:\n",
    "                min_pred = min(selected_dates)\n",
    "            else:\n",
    "                min_pred = '9999-99-99'\n",
    "            if len(default_dates) > 0:\n",
    "                min_default = min(default_dates)\n",
    "            else:\n",
    "                min_default = '9999-99-99'\n",
    "\n",
    "            if min_pred < min_default:\n",
    "                min_pred_idx = selected_dates.index(min_pred)\n",
    "                result_str_lst.append(\n",
    "                    [min_pred + \":\" + ticker_name, prediction[min_pred_idx]])\n",
    "                del selected_dates[min_pred_idx]\n",
    "                prediction = np.delete(prediction, min_pred_idx)\n",
    "            else:\n",
    "                min_default_idx = default_dates.index(min_default)\n",
    "                result_str_lst.append(\n",
    "                    [min_default + \":\" + ticker_name, 0])\n",
    "                del default_dates[min_default_idx]\n",
    "    return result_str_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following settings were used to make predictions on Kaggle:\n",
    "- Random Forest: all tickers, all features, time courses of seven weeks --> Score on kaggle: 79,4 %\n",
    "- Random Forest: all tickers, all features, no time courses --> Score on kaggle: 77,8 %\n",
    "- Random Forest: all tickers, features open and volume, time course of one week --> Score on kaggle: 79,2 %\n",
    "- SVM: 10 tickers, all features, time courses of 90 days, no Grid Search --> Score on kaggle: 71,5 %\n",
    "- SVM: 10 tickers, all features, time courses of 90 days, optimal values were determined with Grid Search --> Score on kaggle: 64,6 %\n",
    "- SVM: 50 tickers, all features, time courses of 90 days, optimal values were determined with Grid Search --> Score on kaggle: 76,5 %\n",
    "- SVM: 150 tickers, all features, time courses of 90 days, optimal values were determined with Grid Search --> Score on kaggle: 80,1 %\n",
    "- KNN: all tickers, all features, time courses of nine weeks --> Score on kaggle: 74,0 %\n",
    "- KNN: all tickers, all features, time courses of 30 days --> Score on kaggle: 71,5 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, the following findings can be derived:\n",
    "- The inclusion of time courses leads to better results. Longer time courses lead to better results than shorter ones. In order to achieve the best possible results, the ideal time courses should be determined.\n",
    "- Furthermore, it can be seen that the inclusion of more tickers leads to better results. The ideal number of tickers can be determined using a learning curve.\n",
    "- The results also suggest that the features are interdependent. For better results and better performance, the ideal features should be determined.\n",
    "- In the example, the use of Grid Search leads to a deterioration in the results. An exact analysis of the reasons is required. One reason for this could be that it is random, since the tickers for the training are selected randomly. A fixed seed can be used to verify the thesis. Another possible reason could be that too few tickers were used in the grid search and that not all parameters were included."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
