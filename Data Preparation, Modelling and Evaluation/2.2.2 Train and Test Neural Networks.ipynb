{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within this notebook we will train and test our 5 different neural networks. \n",
    "\n",
    "The goal is to train each neural network independently to prevent overfitting. Each neural network will be saved in a .h5 and .json file. This files will be later used for prediction by loading the trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import concatenate\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from  pandas  import  DataFrame\n",
    "from  pandas  import  concat\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Dropout, Activation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import tensorflow as tf\n",
    "from keras.backend import tensorflow_backend as K\n",
    "from keras import optimizers\n",
    "from keras import callbacks\n",
    "from keras.constraints import maxnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load data for  1 ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var2(t-150)</th>\n",
       "      <th>var3(t-150)</th>\n",
       "      <th>var2(t-149)</th>\n",
       "      <th>var3(t-149)</th>\n",
       "      <th>var2(t-148)</th>\n",
       "      <th>var3(t-148)</th>\n",
       "      <th>var2(t-147)</th>\n",
       "      <th>var3(t-147)</th>\n",
       "      <th>var2(t-146)</th>\n",
       "      <th>var3(t-146)</th>\n",
       "      <th>...</th>\n",
       "      <th>var2(t+88)</th>\n",
       "      <th>var3(t+88)</th>\n",
       "      <th>var2(t+89)</th>\n",
       "      <th>var3(t+89)</th>\n",
       "      <th>var2(t+90)</th>\n",
       "      <th>var3(t+90)</th>\n",
       "      <th>Y1</th>\n",
       "      <th>Y2</th>\n",
       "      <th>Y3</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.310566</td>\n",
       "      <td>0.075460</td>\n",
       "      <td>0.239299</td>\n",
       "      <td>0.074575</td>\n",
       "      <td>0.423775</td>\n",
       "      <td>0.068674</td>\n",
       "      <td>0.609032</td>\n",
       "      <td>0.062815</td>\n",
       "      <td>0.444209</td>\n",
       "      <td>0.062630</td>\n",
       "      <td>...</td>\n",
       "      <td>0.268147</td>\n",
       "      <td>0.011611</td>\n",
       "      <td>0.298976</td>\n",
       "      <td>0.011636</td>\n",
       "      <td>0.299144</td>\n",
       "      <td>0.010164</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2008-08-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.239299</td>\n",
       "      <td>0.074575</td>\n",
       "      <td>0.423775</td>\n",
       "      <td>0.068674</td>\n",
       "      <td>0.609032</td>\n",
       "      <td>0.062815</td>\n",
       "      <td>0.444209</td>\n",
       "      <td>0.062630</td>\n",
       "      <td>0.531393</td>\n",
       "      <td>0.060859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.298976</td>\n",
       "      <td>0.011636</td>\n",
       "      <td>0.299144</td>\n",
       "      <td>0.010164</td>\n",
       "      <td>0.254234</td>\n",
       "      <td>0.009411</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2008-08-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.423775</td>\n",
       "      <td>0.068674</td>\n",
       "      <td>0.609032</td>\n",
       "      <td>0.062815</td>\n",
       "      <td>0.444209</td>\n",
       "      <td>0.062630</td>\n",
       "      <td>0.531393</td>\n",
       "      <td>0.060859</td>\n",
       "      <td>0.431934</td>\n",
       "      <td>0.063652</td>\n",
       "      <td>...</td>\n",
       "      <td>0.299144</td>\n",
       "      <td>0.010164</td>\n",
       "      <td>0.254234</td>\n",
       "      <td>0.009411</td>\n",
       "      <td>0.314873</td>\n",
       "      <td>0.009418</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2008-08-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.609032</td>\n",
       "      <td>0.062815</td>\n",
       "      <td>0.444209</td>\n",
       "      <td>0.062630</td>\n",
       "      <td>0.531393</td>\n",
       "      <td>0.060859</td>\n",
       "      <td>0.431934</td>\n",
       "      <td>0.063652</td>\n",
       "      <td>0.356585</td>\n",
       "      <td>0.060875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.254234</td>\n",
       "      <td>0.009411</td>\n",
       "      <td>0.314873</td>\n",
       "      <td>0.009418</td>\n",
       "      <td>0.375092</td>\n",
       "      <td>0.006138</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2008-08-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.444209</td>\n",
       "      <td>0.062630</td>\n",
       "      <td>0.531393</td>\n",
       "      <td>0.060859</td>\n",
       "      <td>0.431934</td>\n",
       "      <td>0.063652</td>\n",
       "      <td>0.356585</td>\n",
       "      <td>0.060875</td>\n",
       "      <td>0.316960</td>\n",
       "      <td>0.063061</td>\n",
       "      <td>...</td>\n",
       "      <td>0.314873</td>\n",
       "      <td>0.009418</td>\n",
       "      <td>0.375092</td>\n",
       "      <td>0.006138</td>\n",
       "      <td>0.243912</td>\n",
       "      <td>0.006186</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2008-08-12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 486 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   var2(t-150)  var3(t-150)  var2(t-149)  var3(t-149)  var2(t-148)  \\\n",
       "0     0.310566     0.075460     0.239299     0.074575     0.423775   \n",
       "1     0.239299     0.074575     0.423775     0.068674     0.609032   \n",
       "2     0.423775     0.068674     0.609032     0.062815     0.444209   \n",
       "3     0.609032     0.062815     0.444209     0.062630     0.531393   \n",
       "4     0.444209     0.062630     0.531393     0.060859     0.431934   \n",
       "\n",
       "   var3(t-148)  var2(t-147)  var3(t-147)  var2(t-146)  var3(t-146)  \\\n",
       "0     0.068674     0.609032     0.062815     0.444209     0.062630   \n",
       "1     0.062815     0.444209     0.062630     0.531393     0.060859   \n",
       "2     0.062630     0.531393     0.060859     0.431934     0.063652   \n",
       "3     0.060859     0.431934     0.063652     0.356585     0.060875   \n",
       "4     0.063652     0.356585     0.060875     0.316960     0.063061   \n",
       "\n",
       "      ...      var2(t+88)  var3(t+88)  var2(t+89)  var3(t+89)  var2(t+90)  \\\n",
       "0     ...        0.268147    0.011611    0.298976    0.011636    0.299144   \n",
       "1     ...        0.298976    0.011636    0.299144    0.010164    0.254234   \n",
       "2     ...        0.299144    0.010164    0.254234    0.009411    0.314873   \n",
       "3     ...        0.254234    0.009411    0.314873    0.009418    0.375092   \n",
       "4     ...        0.314873    0.009418    0.375092    0.006138    0.243912   \n",
       "\n",
       "   var3(t+90)  Y1  Y2  Y3        Date  \n",
       "0    0.010164   0   0   1  2008-08-06  \n",
       "1    0.009411   0   0   1  2008-08-07  \n",
       "2    0.009418   0   0   1  2008-08-08  \n",
       "3    0.006138   0   0   1  2008-08-11  \n",
       "4    0.006186   0   0   1  2008-08-12  \n",
       "\n",
       "[5 rows x 486 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Prepared data/Training_set_NN1.csv')\n",
    "df = df.drop('Unnamed: 0',axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Separate into dependend and independant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2368, 482)\n"
     ]
    }
   ],
   "source": [
    "X = df.drop([\"Y1\",\"Y2\",\"Y3\",\"Date\"],axis=1).values\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2368, 3)\n"
     ]
    }
   ],
   "source": [
    "y = df.loc[:, df.columns.intersection(['Y1','Y2','Y3'])].values\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1894, 482) (474, 482) (1894, 3) (474, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n",
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train and test neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1894 samples, validate on 474 samples\n",
      "Epoch 1/200\n",
      " - 1s - loss: 0.2227 - acc: 0.7228 - val_loss: 0.1438 - val_acc: 0.7384\n",
      "Epoch 2/200\n",
      " - 0s - loss: 0.1578 - acc: 0.7254 - val_loss: 0.1366 - val_acc: 0.7384\n",
      "Epoch 3/200\n",
      " - 0s - loss: 0.1493 - acc: 0.7254 - val_loss: 0.1345 - val_acc: 0.7384\n",
      "Epoch 4/200\n",
      " - 0s - loss: 0.1446 - acc: 0.7260 - val_loss: 0.1303 - val_acc: 0.7384\n",
      "Epoch 5/200\n",
      " - 0s - loss: 0.1400 - acc: 0.7260 - val_loss: 0.1264 - val_acc: 0.7384\n",
      "Epoch 6/200\n",
      " - 0s - loss: 0.1353 - acc: 0.7328 - val_loss: 0.1224 - val_acc: 0.7722\n",
      "Epoch 7/200\n",
      " - 0s - loss: 0.1315 - acc: 0.7677 - val_loss: 0.1186 - val_acc: 0.7722\n",
      "Epoch 8/200\n",
      " - 0s - loss: 0.1227 - acc: 0.7746 - val_loss: 0.1099 - val_acc: 0.7806\n",
      "Epoch 9/200\n",
      " - 0s - loss: 0.1137 - acc: 0.8031 - val_loss: 0.1004 - val_acc: 0.8186\n",
      "Epoch 10/200\n",
      " - 0s - loss: 0.1059 - acc: 0.8310 - val_loss: 0.0937 - val_acc: 0.8312\n",
      "Epoch 11/200\n",
      " - 0s - loss: 0.0980 - acc: 0.8432 - val_loss: 0.0926 - val_acc: 0.8418\n",
      "Epoch 12/200\n",
      " - 0s - loss: 0.0975 - acc: 0.8416 - val_loss: 0.0917 - val_acc: 0.8249\n",
      "Epoch 13/200\n",
      " - 0s - loss: 0.0924 - acc: 0.8448 - val_loss: 0.0824 - val_acc: 0.8544\n",
      "Epoch 14/200\n",
      " - 0s - loss: 0.0874 - acc: 0.8548 - val_loss: 0.0803 - val_acc: 0.8586\n",
      "Epoch 15/200\n",
      " - 0s - loss: 0.0858 - acc: 0.8559 - val_loss: 0.0779 - val_acc: 0.8586\n",
      "Epoch 16/200\n",
      " - 0s - loss: 0.0825 - acc: 0.8564 - val_loss: 0.0737 - val_acc: 0.8586\n",
      "Epoch 17/200\n",
      " - 0s - loss: 0.0807 - acc: 0.8601 - val_loss: 0.0744 - val_acc: 0.8650\n",
      "Epoch 18/200\n",
      " - 0s - loss: 0.0780 - acc: 0.8685 - val_loss: 0.0702 - val_acc: 0.8819\n",
      "Epoch 19/200\n",
      " - 0s - loss: 0.0734 - acc: 0.8807 - val_loss: 0.0672 - val_acc: 0.8840\n",
      "Epoch 20/200\n",
      " - 0s - loss: 0.0735 - acc: 0.8733 - val_loss: 0.0633 - val_acc: 0.8840\n",
      "Epoch 21/200\n",
      " - 0s - loss: 0.0694 - acc: 0.8817 - val_loss: 0.0616 - val_acc: 0.9008\n",
      "Epoch 22/200\n",
      " - 0s - loss: 0.0631 - acc: 0.9023 - val_loss: 0.0568 - val_acc: 0.9135\n",
      "Epoch 23/200\n",
      " - 0s - loss: 0.0594 - acc: 0.9124 - val_loss: 0.0615 - val_acc: 0.8903\n",
      "Epoch 24/200\n",
      " - 0s - loss: 0.0572 - acc: 0.9176 - val_loss: 0.0526 - val_acc: 0.9177\n",
      "Epoch 25/200\n",
      " - 0s - loss: 0.0551 - acc: 0.9203 - val_loss: 0.0513 - val_acc: 0.8987\n",
      "Epoch 26/200\n",
      " - 0s - loss: 0.0507 - acc: 0.9293 - val_loss: 0.0461 - val_acc: 0.9114\n",
      "Epoch 27/200\n",
      " - 0s - loss: 0.0524 - acc: 0.9256 - val_loss: 0.0560 - val_acc: 0.9030\n",
      "Epoch 28/200\n",
      " - 0s - loss: 0.0482 - acc: 0.9308 - val_loss: 0.0447 - val_acc: 0.9177\n",
      "Epoch 29/200\n",
      " - 0s - loss: 0.0479 - acc: 0.9308 - val_loss: 0.0450 - val_acc: 0.9241\n",
      "Epoch 30/200\n",
      " - 0s - loss: 0.0440 - acc: 0.9366 - val_loss: 0.0381 - val_acc: 0.9515\n",
      "Epoch 31/200\n",
      " - 0s - loss: 0.0558 - acc: 0.9219 - val_loss: 0.0438 - val_acc: 0.9135\n",
      "Epoch 32/200\n",
      " - 0s - loss: 0.0472 - acc: 0.9340 - val_loss: 0.0381 - val_acc: 0.9473\n",
      "Epoch 33/200\n",
      " - 0s - loss: 0.0414 - acc: 0.9424 - val_loss: 0.0354 - val_acc: 0.9494\n",
      "Epoch 34/200\n",
      " - 0s - loss: 0.0391 - acc: 0.9541 - val_loss: 0.0394 - val_acc: 0.9262\n",
      "Epoch 35/200\n",
      " - 0s - loss: 0.0372 - acc: 0.9567 - val_loss: 0.0361 - val_acc: 0.9304\n",
      "Epoch 36/200\n",
      " - 0s - loss: 0.0378 - acc: 0.9467 - val_loss: 0.0454 - val_acc: 0.9156\n",
      "Epoch 37/200\n",
      " - 0s - loss: 0.0361 - acc: 0.9514 - val_loss: 0.0343 - val_acc: 0.9451\n",
      "Epoch 38/200\n",
      " - 0s - loss: 0.0374 - acc: 0.9509 - val_loss: 0.0329 - val_acc: 0.9473\n",
      "Epoch 39/200\n",
      " - 0s - loss: 0.0372 - acc: 0.9514 - val_loss: 0.0416 - val_acc: 0.9177\n",
      "Epoch 40/200\n",
      " - 0s - loss: 0.0360 - acc: 0.9509 - val_loss: 0.0342 - val_acc: 0.9451\n",
      "Epoch 41/200\n",
      " - 0s - loss: 0.0311 - acc: 0.9609 - val_loss: 0.0284 - val_acc: 0.9515\n",
      "Epoch 42/200\n",
      " - 0s - loss: 0.0341 - acc: 0.9588 - val_loss: 0.0396 - val_acc: 0.9304\n",
      "Epoch 43/200\n",
      " - 0s - loss: 0.0393 - acc: 0.9461 - val_loss: 0.0439 - val_acc: 0.9177\n",
      "Epoch 44/200\n",
      " - 0s - loss: 0.0352 - acc: 0.9535 - val_loss: 0.0316 - val_acc: 0.9430\n",
      "Epoch 45/200\n",
      " - 0s - loss: 0.0339 - acc: 0.9535 - val_loss: 0.0320 - val_acc: 0.9430\n",
      "Epoch 46/200\n",
      " - 0s - loss: 0.0309 - acc: 0.9630 - val_loss: 0.0289 - val_acc: 0.9557\n",
      "Epoch 47/200\n",
      " - 0s - loss: 0.0421 - acc: 0.9430 - val_loss: 0.0508 - val_acc: 0.9135\n",
      "Epoch 48/200\n",
      " - 0s - loss: 0.0468 - acc: 0.9271 - val_loss: 0.0376 - val_acc: 0.9304\n",
      "Epoch 49/200\n",
      " - 0s - loss: 0.0312 - acc: 0.9641 - val_loss: 0.0319 - val_acc: 0.9409\n",
      "Epoch 50/200\n",
      " - 0s - loss: 0.0320 - acc: 0.9567 - val_loss: 0.0342 - val_acc: 0.9451\n",
      "Epoch 51/200\n",
      " - 0s - loss: 0.0306 - acc: 0.9593 - val_loss: 0.0306 - val_acc: 0.9536\n",
      "Epoch 52/200\n",
      " - 0s - loss: 0.0308 - acc: 0.9588 - val_loss: 0.0291 - val_acc: 0.9536\n",
      "Epoch 53/200\n",
      " - 0s - loss: 0.0297 - acc: 0.9652 - val_loss: 0.0292 - val_acc: 0.9515\n",
      "Epoch 54/200\n",
      " - 0s - loss: 0.0285 - acc: 0.9662 - val_loss: 0.0358 - val_acc: 0.9409\n",
      "Epoch 55/200\n",
      " - 0s - loss: 0.0290 - acc: 0.9625 - val_loss: 0.0365 - val_acc: 0.9346\n",
      "Epoch 56/200\n",
      " - 0s - loss: 0.0315 - acc: 0.9572 - val_loss: 0.0298 - val_acc: 0.9473\n",
      "Epoch 57/200\n",
      " - 0s - loss: 0.0283 - acc: 0.9646 - val_loss: 0.0313 - val_acc: 0.9451\n",
      "Epoch 58/200\n",
      " - 0s - loss: 0.0285 - acc: 0.9620 - val_loss: 0.0295 - val_acc: 0.9515\n",
      "Epoch 59/200\n",
      " - 0s - loss: 0.0239 - acc: 0.9725 - val_loss: 0.0379 - val_acc: 0.9241\n",
      "Epoch 60/200\n",
      " - 0s - loss: 0.0267 - acc: 0.9673 - val_loss: 0.0301 - val_acc: 0.9515\n",
      "Epoch 61/200\n",
      " - 0s - loss: 0.0257 - acc: 0.9652 - val_loss: 0.0301 - val_acc: 0.9494\n",
      "Epoch 62/200\n",
      " - 0s - loss: 0.0241 - acc: 0.9699 - val_loss: 0.0275 - val_acc: 0.9494\n",
      "Epoch 63/200\n",
      " - 0s - loss: 0.0245 - acc: 0.9688 - val_loss: 0.0277 - val_acc: 0.9536\n",
      "Epoch 64/200\n",
      " - 0s - loss: 0.0232 - acc: 0.9720 - val_loss: 0.0378 - val_acc: 0.9304\n",
      "Epoch 65/200\n",
      " - 0s - loss: 0.0230 - acc: 0.9731 - val_loss: 0.0299 - val_acc: 0.9494\n",
      "Epoch 66/200\n",
      " - 0s - loss: 0.0244 - acc: 0.9699 - val_loss: 0.0287 - val_acc: 0.9515\n",
      "Epoch 67/200\n",
      " - 0s - loss: 0.0241 - acc: 0.9694 - val_loss: 0.0349 - val_acc: 0.9367\n",
      "Epoch 68/200\n",
      " - 0s - loss: 0.0267 - acc: 0.9641 - val_loss: 0.0308 - val_acc: 0.9409\n",
      "Epoch 69/200\n",
      " - 0s - loss: 0.0220 - acc: 0.9757 - val_loss: 0.0277 - val_acc: 0.9515\n",
      "Epoch 70/200\n",
      " - 0s - loss: 0.0225 - acc: 0.9736 - val_loss: 0.0302 - val_acc: 0.9430\n",
      "Epoch 71/200\n",
      " - 0s - loss: 0.0203 - acc: 0.9789 - val_loss: 0.0262 - val_acc: 0.9578\n",
      "Epoch 72/200\n",
      " - 0s - loss: 0.0305 - acc: 0.9593 - val_loss: 0.0529 - val_acc: 0.9072\n",
      "Epoch 73/200\n",
      " - 0s - loss: 0.0500 - acc: 0.9224 - val_loss: 0.0522 - val_acc: 0.9135\n",
      "Epoch 74/200\n",
      " - 0s - loss: 0.0473 - acc: 0.9340 - val_loss: 0.0543 - val_acc: 0.9051\n",
      "Epoch 75/200\n",
      " - 0s - loss: 0.0425 - acc: 0.9356 - val_loss: 0.0457 - val_acc: 0.9156\n",
      "Epoch 76/200\n",
      " - 0s - loss: 0.0379 - acc: 0.9382 - val_loss: 0.0446 - val_acc: 0.9135\n",
      "Epoch 77/200\n",
      " - 0s - loss: 0.0313 - acc: 0.9572 - val_loss: 0.0352 - val_acc: 0.9388\n",
      "Epoch 78/200\n",
      " - 0s - loss: 0.0243 - acc: 0.9757 - val_loss: 0.0351 - val_acc: 0.9388\n",
      "Epoch 79/200\n",
      " - 0s - loss: 0.0236 - acc: 0.9710 - val_loss: 0.0299 - val_acc: 0.9536\n",
      "Epoch 80/200\n",
      " - 0s - loss: 0.0212 - acc: 0.9757 - val_loss: 0.0305 - val_acc: 0.9473\n",
      "Epoch 81/200\n",
      " - 0s - loss: 0.0236 - acc: 0.9715 - val_loss: 0.0319 - val_acc: 0.9494\n",
      "Epoch 82/200\n",
      " - 0s - loss: 0.0215 - acc: 0.9736 - val_loss: 0.0304 - val_acc: 0.9515\n",
      "Epoch 83/200\n",
      " - 0s - loss: 0.0215 - acc: 0.9741 - val_loss: 0.0271 - val_acc: 0.9515\n",
      "Epoch 84/200\n",
      " - 0s - loss: 0.0217 - acc: 0.9704 - val_loss: 0.0400 - val_acc: 0.9241\n",
      "Epoch 85/200\n",
      " - 0s - loss: 0.0285 - acc: 0.9604 - val_loss: 0.0308 - val_acc: 0.9451\n",
      "Epoch 86/200\n",
      " - 0s - loss: 0.0225 - acc: 0.9704 - val_loss: 0.0304 - val_acc: 0.9494\n",
      "Epoch 87/200\n",
      " - 0s - loss: 0.0208 - acc: 0.9773 - val_loss: 0.0321 - val_acc: 0.9430\n",
      "Epoch 88/200\n",
      " - 0s - loss: 0.0198 - acc: 0.9784 - val_loss: 0.0298 - val_acc: 0.9515\n",
      "Epoch 89/200\n",
      " - 0s - loss: 0.0187 - acc: 0.9768 - val_loss: 0.0314 - val_acc: 0.9473\n",
      "Epoch 90/200\n",
      " - 0s - loss: 0.0182 - acc: 0.9815 - val_loss: 0.0302 - val_acc: 0.9494\n",
      "Epoch 91/200\n",
      " - 0s - loss: 0.0183 - acc: 0.9799 - val_loss: 0.0302 - val_acc: 0.9494\n",
      "Epoch 92/200\n",
      " - 0s - loss: 0.0232 - acc: 0.9688 - val_loss: 0.0328 - val_acc: 0.9430\n",
      "Epoch 93/200\n",
      " - 0s - loss: 0.0213 - acc: 0.9725 - val_loss: 0.0276 - val_acc: 0.9515\n",
      "Epoch 94/200\n",
      " - 0s - loss: 0.0234 - acc: 0.9678 - val_loss: 0.0439 - val_acc: 0.9156\n",
      "Epoch 95/200\n",
      " - 0s - loss: 0.0194 - acc: 0.9815 - val_loss: 0.0322 - val_acc: 0.9473\n",
      "Epoch 96/200\n",
      " - 0s - loss: 0.0200 - acc: 0.9768 - val_loss: 0.0275 - val_acc: 0.9515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/200\n",
      " - 0s - loss: 0.0178 - acc: 0.9805 - val_loss: 0.0299 - val_acc: 0.9557\n",
      "Epoch 98/200\n",
      " - 0s - loss: 0.0224 - acc: 0.9710 - val_loss: 0.0311 - val_acc: 0.9473\n",
      "Epoch 99/200\n",
      " - 0s - loss: 0.0238 - acc: 0.9636 - val_loss: 0.0306 - val_acc: 0.9473\n",
      "Epoch 100/200\n",
      " - 0s - loss: 0.0179 - acc: 0.9810 - val_loss: 0.0482 - val_acc: 0.9135\n",
      "Epoch 101/200\n",
      " - 0s - loss: 0.0176 - acc: 0.9799 - val_loss: 0.0322 - val_acc: 0.9430\n",
      "Epoch 102/200\n",
      " - 0s - loss: 0.0186 - acc: 0.9794 - val_loss: 0.0307 - val_acc: 0.9430\n",
      "Epoch 103/200\n",
      " - 0s - loss: 0.0159 - acc: 0.9831 - val_loss: 0.0265 - val_acc: 0.9578\n",
      "Epoch 104/200\n",
      " - 0s - loss: 0.0188 - acc: 0.9773 - val_loss: 0.0317 - val_acc: 0.9388\n",
      "Epoch 105/200\n",
      " - 0s - loss: 0.0179 - acc: 0.9810 - val_loss: 0.0277 - val_acc: 0.9536\n",
      "Epoch 106/200\n",
      " - 0s - loss: 0.0363 - acc: 0.9493 - val_loss: 0.0533 - val_acc: 0.9114\n",
      "Epoch 107/200\n",
      " - 0s - loss: 0.0375 - acc: 0.9456 - val_loss: 0.0357 - val_acc: 0.9388\n",
      "Epoch 108/200\n",
      " - 0s - loss: 0.0195 - acc: 0.9810 - val_loss: 0.0322 - val_acc: 0.9409\n",
      "Epoch 109/200\n",
      " - 0s - loss: 0.0191 - acc: 0.9762 - val_loss: 0.0298 - val_acc: 0.9473\n",
      "Epoch 110/200\n",
      " - 0s - loss: 0.0197 - acc: 0.9773 - val_loss: 0.0396 - val_acc: 0.9304\n",
      "Epoch 111/200\n",
      " - 0s - loss: 0.0197 - acc: 0.9778 - val_loss: 0.0307 - val_acc: 0.9494\n",
      "Epoch 112/200\n",
      " - 0s - loss: 0.0200 - acc: 0.9762 - val_loss: 0.0317 - val_acc: 0.9430\n",
      "Epoch 113/200\n",
      " - 0s - loss: 0.0238 - acc: 0.9678 - val_loss: 0.0425 - val_acc: 0.9262\n",
      "Epoch 114/200\n",
      " - 0s - loss: 0.0203 - acc: 0.9741 - val_loss: 0.0323 - val_acc: 0.9473\n",
      "Epoch 115/200\n",
      " - 0s - loss: 0.0174 - acc: 0.9805 - val_loss: 0.0284 - val_acc: 0.9430\n",
      "Epoch 116/200\n",
      " - 0s - loss: 0.0184 - acc: 0.9810 - val_loss: 0.0339 - val_acc: 0.9388\n",
      "Epoch 117/200\n",
      " - 0s - loss: 0.0184 - acc: 0.9768 - val_loss: 0.0300 - val_acc: 0.9494\n",
      "Epoch 118/200\n",
      " - 0s - loss: 0.0158 - acc: 0.9826 - val_loss: 0.0305 - val_acc: 0.9430\n",
      "Epoch 119/200\n",
      " - 0s - loss: 0.0193 - acc: 0.9762 - val_loss: 0.0338 - val_acc: 0.9451\n",
      "Epoch 120/200\n",
      " - 0s - loss: 0.0177 - acc: 0.9815 - val_loss: 0.0294 - val_acc: 0.9515\n",
      "Epoch 121/200\n",
      " - 0s - loss: 0.0155 - acc: 0.9842 - val_loss: 0.0318 - val_acc: 0.9473\n",
      "Epoch 122/200\n",
      " - 0s - loss: 0.0158 - acc: 0.9820 - val_loss: 0.0301 - val_acc: 0.9473\n",
      "Epoch 123/200\n",
      " - 0s - loss: 0.0172 - acc: 0.9784 - val_loss: 0.0370 - val_acc: 0.9325\n",
      "Epoch 124/200\n",
      " - 0s - loss: 0.0501 - acc: 0.9261 - val_loss: 0.0620 - val_acc: 0.8966\n",
      "Epoch 125/200\n",
      " - 0s - loss: 0.0456 - acc: 0.9329 - val_loss: 0.0425 - val_acc: 0.9241\n",
      "Epoch 126/200\n",
      " - 0s - loss: 0.0356 - acc: 0.9456 - val_loss: 0.0339 - val_acc: 0.9388\n",
      "Epoch 127/200\n",
      " - 0s - loss: 0.0295 - acc: 0.9556 - val_loss: 0.0297 - val_acc: 0.9473\n",
      "Epoch 128/200\n",
      " - 0s - loss: 0.0218 - acc: 0.9725 - val_loss: 0.0291 - val_acc: 0.9494\n",
      "Epoch 129/200\n",
      " - 0s - loss: 0.0177 - acc: 0.9799 - val_loss: 0.0293 - val_acc: 0.9473\n",
      "Epoch 130/200\n",
      " - 0s - loss: 0.0237 - acc: 0.9646 - val_loss: 0.0335 - val_acc: 0.9367\n",
      "Epoch 131/200\n",
      " - 0s - loss: 0.0167 - acc: 0.9805 - val_loss: 0.0317 - val_acc: 0.9494\n",
      "Epoch 132/200\n",
      " - 0s - loss: 0.0160 - acc: 0.9826 - val_loss: 0.0303 - val_acc: 0.9430\n",
      "Epoch 133/200\n",
      " - 0s - loss: 0.0223 - acc: 0.9720 - val_loss: 0.0409 - val_acc: 0.9262\n",
      "Epoch 134/200\n",
      " - 0s - loss: 0.0217 - acc: 0.9731 - val_loss: 0.0284 - val_acc: 0.9557\n",
      "Epoch 135/200\n",
      " - 0s - loss: 0.0147 - acc: 0.9842 - val_loss: 0.0270 - val_acc: 0.9536\n",
      "Epoch 136/200\n",
      " - 0s - loss: 0.0135 - acc: 0.9868 - val_loss: 0.0277 - val_acc: 0.9494\n",
      "Epoch 137/200\n",
      " - 0s - loss: 0.0133 - acc: 0.9857 - val_loss: 0.0314 - val_acc: 0.9451\n",
      "Epoch 138/200\n",
      " - 0s - loss: 0.0144 - acc: 0.9842 - val_loss: 0.0281 - val_acc: 0.9536\n",
      "Epoch 139/200\n",
      " - 0s - loss: 0.0284 - acc: 0.9556 - val_loss: 0.0526 - val_acc: 0.9072\n",
      "Epoch 140/200\n",
      " - 0s - loss: 0.0227 - acc: 0.9694 - val_loss: 0.0268 - val_acc: 0.9557\n",
      "Epoch 141/200\n",
      " - 0s - loss: 0.0164 - acc: 0.9820 - val_loss: 0.0253 - val_acc: 0.9557\n",
      "Epoch 142/200\n",
      " - 0s - loss: 0.0136 - acc: 0.9852 - val_loss: 0.0312 - val_acc: 0.9451\n",
      "Epoch 143/200\n",
      " - 0s - loss: 0.0190 - acc: 0.9731 - val_loss: 0.0325 - val_acc: 0.9473\n",
      "Epoch 144/200\n",
      " - 0s - loss: 0.0183 - acc: 0.9757 - val_loss: 0.0279 - val_acc: 0.9494\n",
      "Epoch 145/200\n",
      " - 0s - loss: 0.0138 - acc: 0.9831 - val_loss: 0.0300 - val_acc: 0.9494\n",
      "Epoch 146/200\n",
      " - 0s - loss: 0.0231 - acc: 0.9662 - val_loss: 0.0362 - val_acc: 0.9346\n",
      "Epoch 147/200\n",
      " - 0s - loss: 0.0189 - acc: 0.9736 - val_loss: 0.0334 - val_acc: 0.9451\n",
      "Epoch 148/200\n",
      " - 0s - loss: 0.0171 - acc: 0.9794 - val_loss: 0.0325 - val_acc: 0.9409\n",
      "Epoch 149/200\n",
      " - 0s - loss: 0.0173 - acc: 0.9805 - val_loss: 0.0309 - val_acc: 0.9451\n",
      "Epoch 150/200\n",
      " - 0s - loss: 0.0217 - acc: 0.9699 - val_loss: 0.0408 - val_acc: 0.9283\n",
      "Epoch 151/200\n",
      " - 0s - loss: 0.0184 - acc: 0.9762 - val_loss: 0.0317 - val_acc: 0.9473\n",
      "Epoch 152/200\n",
      " - 0s - loss: 0.0195 - acc: 0.9736 - val_loss: 0.0321 - val_acc: 0.9388\n",
      "Epoch 153/200\n",
      " - 0s - loss: 0.0161 - acc: 0.9784 - val_loss: 0.0276 - val_acc: 0.9536\n",
      "Epoch 154/200\n",
      " - 0s - loss: 0.0144 - acc: 0.9857 - val_loss: 0.0304 - val_acc: 0.9473\n",
      "Epoch 155/200\n",
      " - 0s - loss: 0.0133 - acc: 0.9852 - val_loss: 0.0269 - val_acc: 0.9557\n",
      "Epoch 156/200\n",
      " - 0s - loss: 0.0127 - acc: 0.9873 - val_loss: 0.0282 - val_acc: 0.9536\n",
      "Epoch 157/200\n",
      " - 0s - loss: 0.0148 - acc: 0.9820 - val_loss: 0.0310 - val_acc: 0.9515\n",
      "Epoch 158/200\n",
      " - 0s - loss: 0.0129 - acc: 0.9863 - val_loss: 0.0261 - val_acc: 0.9578\n",
      "Epoch 159/200\n",
      " - 0s - loss: 0.0161 - acc: 0.9820 - val_loss: 0.0305 - val_acc: 0.9494\n",
      "Epoch 160/200\n",
      " - 0s - loss: 0.0171 - acc: 0.9762 - val_loss: 0.0410 - val_acc: 0.9219\n",
      "Epoch 161/200\n",
      " - 0s - loss: 0.0165 - acc: 0.9805 - val_loss: 0.0268 - val_acc: 0.9557\n",
      "Epoch 162/200\n",
      " - 0s - loss: 0.0133 - acc: 0.9857 - val_loss: 0.0278 - val_acc: 0.9536\n",
      "Epoch 163/200\n",
      " - 0s - loss: 0.0152 - acc: 0.9836 - val_loss: 0.0299 - val_acc: 0.9536\n",
      "Epoch 164/200\n",
      " - 0s - loss: 0.0142 - acc: 0.9852 - val_loss: 0.0275 - val_acc: 0.9536\n",
      "Epoch 165/200\n",
      " - 0s - loss: 0.0149 - acc: 0.9831 - val_loss: 0.0284 - val_acc: 0.9536\n",
      "Epoch 166/200\n",
      " - 0s - loss: 0.0148 - acc: 0.9826 - val_loss: 0.0341 - val_acc: 0.9346\n",
      "Epoch 167/200\n",
      " - 0s - loss: 0.0163 - acc: 0.9794 - val_loss: 0.0330 - val_acc: 0.9451\n",
      "Epoch 168/200\n",
      " - 0s - loss: 0.0130 - acc: 0.9852 - val_loss: 0.0314 - val_acc: 0.9451\n",
      "Epoch 169/200\n",
      " - 0s - loss: 0.0165 - acc: 0.9805 - val_loss: 0.0326 - val_acc: 0.9430\n",
      "Epoch 170/200\n",
      " - 0s - loss: 0.0144 - acc: 0.9820 - val_loss: 0.0312 - val_acc: 0.9473\n",
      "Epoch 171/200\n",
      " - 0s - loss: 0.0185 - acc: 0.9747 - val_loss: 0.0379 - val_acc: 0.9367\n",
      "Epoch 172/200\n",
      " - 0s - loss: 0.0138 - acc: 0.9847 - val_loss: 0.0267 - val_acc: 0.9557\n",
      "Epoch 173/200\n",
      " - 0s - loss: 0.0150 - acc: 0.9773 - val_loss: 0.0389 - val_acc: 0.9304\n",
      "Epoch 174/200\n",
      " - 0s - loss: 0.0132 - acc: 0.9836 - val_loss: 0.0256 - val_acc: 0.9536\n",
      "Epoch 175/200\n",
      " - 0s - loss: 0.0120 - acc: 0.9873 - val_loss: 0.0314 - val_acc: 0.9473\n",
      "Epoch 176/200\n",
      " - 0s - loss: 0.0127 - acc: 0.9863 - val_loss: 0.0274 - val_acc: 0.9557\n",
      "Epoch 177/200\n",
      " - 0s - loss: 0.0166 - acc: 0.9773 - val_loss: 0.0443 - val_acc: 0.9241\n",
      "Epoch 178/200\n",
      " - 0s - loss: 0.0239 - acc: 0.9609 - val_loss: 0.0253 - val_acc: 0.9557\n",
      "Epoch 179/200\n",
      " - 0s - loss: 0.0139 - acc: 0.9863 - val_loss: 0.0247 - val_acc: 0.9620\n",
      "Epoch 180/200\n",
      " - 0s - loss: 0.0120 - acc: 0.9879 - val_loss: 0.0274 - val_acc: 0.9536\n",
      "Epoch 181/200\n",
      " - 0s - loss: 0.0151 - acc: 0.9805 - val_loss: 0.0264 - val_acc: 0.9557\n",
      "Epoch 182/200\n",
      " - 0s - loss: 0.0170 - acc: 0.9784 - val_loss: 0.0267 - val_acc: 0.9578\n",
      "Epoch 183/200\n",
      " - 0s - loss: 0.0129 - acc: 0.9847 - val_loss: 0.0294 - val_acc: 0.9494\n",
      "Epoch 184/200\n",
      " - 0s - loss: 0.0137 - acc: 0.9852 - val_loss: 0.0282 - val_acc: 0.9557\n",
      "Epoch 185/200\n",
      " - 0s - loss: 0.0125 - acc: 0.9852 - val_loss: 0.0292 - val_acc: 0.9494\n",
      "Epoch 186/200\n",
      " - 0s - loss: 0.0118 - acc: 0.9889 - val_loss: 0.0293 - val_acc: 0.9536\n",
      "Epoch 187/200\n",
      " - 0s - loss: 0.0136 - acc: 0.9847 - val_loss: 0.0268 - val_acc: 0.9557\n",
      "Epoch 188/200\n",
      " - 0s - loss: 0.0105 - acc: 0.9884 - val_loss: 0.0275 - val_acc: 0.9536\n",
      "Epoch 189/200\n",
      " - 0s - loss: 0.0112 - acc: 0.9900 - val_loss: 0.0269 - val_acc: 0.9557\n",
      "Epoch 190/200\n",
      " - 0s - loss: 0.0117 - acc: 0.9868 - val_loss: 0.0451 - val_acc: 0.9304\n",
      "Epoch 191/200\n",
      " - 0s - loss: 0.0225 - acc: 0.9683 - val_loss: 0.0338 - val_acc: 0.9283\n",
      "Epoch 192/200\n",
      " - 0s - loss: 0.0220 - acc: 0.9657 - val_loss: 0.0371 - val_acc: 0.9388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193/200\n",
      " - 0s - loss: 0.0124 - acc: 0.9863 - val_loss: 0.0248 - val_acc: 0.9578\n",
      "Epoch 194/200\n",
      " - 0s - loss: 0.0107 - acc: 0.9894 - val_loss: 0.0280 - val_acc: 0.9536\n",
      "Epoch 195/200\n",
      " - 0s - loss: 0.0122 - acc: 0.9863 - val_loss: 0.0307 - val_acc: 0.9451\n",
      "Epoch 196/200\n",
      " - 0s - loss: 0.0144 - acc: 0.9799 - val_loss: 0.0285 - val_acc: 0.9494\n",
      "Epoch 197/200\n",
      " - 0s - loss: 0.0129 - acc: 0.9852 - val_loss: 0.0263 - val_acc: 0.9578\n",
      "Epoch 198/200\n",
      " - 0s - loss: 0.0129 - acc: 0.9852 - val_loss: 0.0329 - val_acc: 0.9409\n",
      "Epoch 199/200\n",
      " - 0s - loss: 0.0111 - acc: 0.9884 - val_loss: 0.0243 - val_acc: 0.9599\n",
      "Epoch 200/200\n",
      " - 0s - loss: 0.0096 - acc: 0.9926 - val_loss: 0.0257 - val_acc: 0.9578\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(500, input_dim=482, activation=\"relu\", kernel_initializer=\"normal\", \n",
    "                    kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(100, activation=\"relu\", kernel_initializer=\"normal\", \n",
    "                    kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(50, activation=\"relu\", kernel_initializer=\"normal\", \n",
    "                    kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(10, activation=\"relu\", kernel_initializer=\"normal\", \n",
    "                    kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(3, activation=\"relu\", kernel_initializer=\"normal\"))\n",
    "\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=\"Adam\", metrics=['accuracy'])\n",
    "                \n",
    "Neural_model = model.fit(X_train, y_train, epochs=200, batch_size=100, verbose=2,\n",
    "                         validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(Neural_model.history['acc'], label='train')\n",
    "plt.plot(Neural_model.history['val_acc'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Prevent overfitting:</h3>\n",
    "    \n",
    "As you can see in the chart above the training accuracy increases slightly after epoch 50 but our test accuracy remains roughly at the same level. Based on this it is recognizable that our model starts to overfit after epoch 50. To receive a good neural network which will generalize good we have to stop our neural network during training before it starts overfitting. \n",
    "The difficult part is that each neural network will have different training results. This will make it impossible to define a fix number of epochs. \n",
    "\n",
    "To stop each neural network during training before it starts overfitting we will use the callback method \"EarlyStopping\" from Tensorflow which will monitor the loss of our test set. If this value will not improve over a few epochs the callback will stop our neural network from training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Train and test neural network with EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1894 samples, validate on 474 samples\n",
      "Epoch 1/200\n",
      " - 1s - loss: 0.2289 - acc: 0.7043 - val_loss: 0.1414 - val_acc: 0.7384\n",
      "Epoch 2/200\n",
      " - 0s - loss: 0.1546 - acc: 0.7260 - val_loss: 0.1354 - val_acc: 0.7384\n",
      "Epoch 3/200\n",
      " - 0s - loss: 0.1449 - acc: 0.7260 - val_loss: 0.1306 - val_acc: 0.7384\n",
      "Epoch 4/200\n",
      " - 0s - loss: 0.1409 - acc: 0.7429 - val_loss: 0.1269 - val_acc: 0.7384\n",
      "Epoch 5/200\n",
      " - 0s - loss: 0.1364 - acc: 0.7571 - val_loss: 0.1215 - val_acc: 0.7743\n",
      "Epoch 6/200\n",
      " - 0s - loss: 0.1290 - acc: 0.7687 - val_loss: 0.1129 - val_acc: 0.7869\n",
      "Epoch 7/200\n",
      " - 0s - loss: 0.1209 - acc: 0.7825 - val_loss: 0.1043 - val_acc: 0.7932\n",
      "Epoch 8/200\n",
      " - 0s - loss: 0.1104 - acc: 0.8110 - val_loss: 0.0973 - val_acc: 0.8333\n",
      "Epoch 9/200\n",
      " - 0s - loss: 0.1058 - acc: 0.8152 - val_loss: 0.0990 - val_acc: 0.8333\n",
      "Epoch 10/200\n",
      " - 0s - loss: 0.0992 - acc: 0.8321 - val_loss: 0.0925 - val_acc: 0.8333\n",
      "Epoch 11/200\n",
      " - 0s - loss: 0.0935 - acc: 0.8395 - val_loss: 0.0842 - val_acc: 0.8523\n",
      "Epoch 12/200\n",
      " - 0s - loss: 0.0902 - acc: 0.8474 - val_loss: 0.0812 - val_acc: 0.8586\n",
      "Epoch 13/200\n",
      " - 0s - loss: 0.0875 - acc: 0.8421 - val_loss: 0.0802 - val_acc: 0.8586\n",
      "Epoch 14/200\n",
      " - 0s - loss: 0.0859 - acc: 0.8516 - val_loss: 0.0783 - val_acc: 0.8608\n",
      "Epoch 15/200\n",
      " - 0s - loss: 0.0842 - acc: 0.8553 - val_loss: 0.0767 - val_acc: 0.8586\n",
      "Epoch 16/200\n",
      " - 0s - loss: 0.0830 - acc: 0.8574 - val_loss: 0.0737 - val_acc: 0.8608\n",
      "Epoch 17/200\n",
      " - 0s - loss: 0.0764 - acc: 0.8659 - val_loss: 0.0706 - val_acc: 0.8671\n",
      "Epoch 18/200\n",
      " - 0s - loss: 0.0721 - acc: 0.8633 - val_loss: 0.0676 - val_acc: 0.8629\n",
      "Epoch 19/200\n",
      " - 0s - loss: 0.0701 - acc: 0.8728 - val_loss: 0.0638 - val_acc: 0.8734\n",
      "Epoch 20/200\n",
      " - 0s - loss: 0.0700 - acc: 0.8860 - val_loss: 0.0676 - val_acc: 0.9030\n",
      "Epoch 21/200\n",
      " - 0s - loss: 0.0640 - acc: 0.9134 - val_loss: 0.0609 - val_acc: 0.9093\n",
      "Epoch 22/200\n",
      " - 0s - loss: 0.0630 - acc: 0.9102 - val_loss: 0.0590 - val_acc: 0.9135\n",
      "Epoch 23/200\n",
      " - 0s - loss: 0.0594 - acc: 0.9139 - val_loss: 0.0530 - val_acc: 0.9008\n",
      "Epoch 24/200\n",
      " - 0s - loss: 0.0531 - acc: 0.9282 - val_loss: 0.0540 - val_acc: 0.9030\n",
      "Epoch 25/200\n",
      " - 0s - loss: 0.0535 - acc: 0.9256 - val_loss: 0.0480 - val_acc: 0.9135\n",
      "Epoch 26/200\n",
      " - 0s - loss: 0.0492 - acc: 0.9298 - val_loss: 0.0474 - val_acc: 0.9114\n",
      "Epoch 27/200\n",
      " - 0s - loss: 0.0498 - acc: 0.9266 - val_loss: 0.0538 - val_acc: 0.8966\n",
      "Epoch 28/200\n",
      " - 0s - loss: 0.0555 - acc: 0.9176 - val_loss: 0.0632 - val_acc: 0.8713\n",
      "Epoch 29/200\n",
      " - 0s - loss: 0.0548 - acc: 0.9161 - val_loss: 0.0535 - val_acc: 0.9177\n",
      "Epoch 30/200\n",
      " - 0s - loss: 0.0514 - acc: 0.9335 - val_loss: 0.0558 - val_acc: 0.9093\n",
      "Epoch 31/200\n",
      " - 0s - loss: 0.0546 - acc: 0.9234 - val_loss: 0.0432 - val_acc: 0.9367\n",
      "Epoch 32/200\n",
      " - 0s - loss: 0.0439 - acc: 0.9419 - val_loss: 0.0379 - val_acc: 0.9346\n",
      "Epoch 33/200\n",
      " - 0s - loss: 0.0387 - acc: 0.9483 - val_loss: 0.0512 - val_acc: 0.9030\n",
      "Epoch 34/200\n",
      " - 0s - loss: 0.0408 - acc: 0.9435 - val_loss: 0.0422 - val_acc: 0.9325\n",
      "Epoch 35/200\n",
      " - 0s - loss: 0.0381 - acc: 0.9488 - val_loss: 0.0446 - val_acc: 0.9177\n",
      "Epoch 36/200\n",
      " - 0s - loss: 0.0392 - acc: 0.9483 - val_loss: 0.0343 - val_acc: 0.9430\n",
      "Epoch 37/200\n",
      " - 0s - loss: 0.0421 - acc: 0.9419 - val_loss: 0.0501 - val_acc: 0.9093\n",
      "Epoch 38/200\n",
      " - 0s - loss: 0.0370 - acc: 0.9525 - val_loss: 0.0325 - val_acc: 0.9409\n",
      "Epoch 39/200\n",
      " - 0s - loss: 0.0360 - acc: 0.9525 - val_loss: 0.0360 - val_acc: 0.9388\n",
      "Epoch 40/200\n",
      " - 0s - loss: 0.0331 - acc: 0.9562 - val_loss: 0.0425 - val_acc: 0.9198\n",
      "Epoch 41/200\n",
      " - 0s - loss: 0.0362 - acc: 0.9472 - val_loss: 0.0308 - val_acc: 0.9536\n",
      "Epoch 42/200\n",
      " - 0s - loss: 0.0316 - acc: 0.9578 - val_loss: 0.0317 - val_acc: 0.9473\n",
      "Epoch 43/200\n",
      " - 0s - loss: 0.0307 - acc: 0.9615 - val_loss: 0.0323 - val_acc: 0.9515\n",
      "Epoch 44/200\n",
      " - 0s - loss: 0.0327 - acc: 0.9615 - val_loss: 0.0320 - val_acc: 0.9515\n",
      "Epoch 45/200\n",
      " - 0s - loss: 0.0324 - acc: 0.9588 - val_loss: 0.0450 - val_acc: 0.9114\n",
      "Epoch 46/200\n",
      " - 0s - loss: 0.0353 - acc: 0.9525 - val_loss: 0.0321 - val_acc: 0.9515\n",
      "Epoch 00046: early stopping\n"
     ]
    }
   ],
   "source": [
    "earlystopping = callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, verbose=4)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(500, input_dim=482, activation=\"relu\", kernel_initializer=\"normal\", \n",
    "                    kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(100, activation=\"relu\", kernel_initializer=\"normal\", \n",
    "                    kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(50, activation=\"relu\", kernel_initializer=\"normal\", \n",
    "                    kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(10, activation=\"relu\", kernel_initializer=\"normal\", \n",
    "                    kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(3, activation=\"relu\", kernel_initializer=\"normal\"))\n",
    "\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=\"Adam\", metrics=['accuracy'])\n",
    "                \n",
    "Neural_model = model.fit(X_train, y_train, epochs=200, batch_size=100, verbose=2,\n",
    "                         validation_data=(X_test, y_test), callbacks = [earlystopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the output field above our callback stopped our neural network after the 46 epoch before our model starts overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: [0.019521349851718644, 0.9751847940865892]\n",
      "Test Score: [0.032130136174240195, 0.9514767917399668]\n"
     ]
    }
   ],
   "source": [
    "trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
    "print('Train Score: ' + str(trainScore))\n",
    "testScore = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Score: ' + str(testScore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we archieved really good training and test results. A test accuracy of around 95% is a pretty good result. To use this trained network for predictions later on, we will now save the model and the weights of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Save settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 0 was saved successfully\n",
      "structure 0 was saved successfully\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,1):\n",
    "    # Serialize the model to json\n",
    "    model_json = model.to_json()\n",
    "    json_file = open(\"2.2.2.1 Settings neural networks/model\" + str(i) + \".json\", 'w')\n",
    "    json_file.write(model_json)\n",
    "    json_file.close()\n",
    "    print(\"model \" + str(i) + \" was saved successfully\")\n",
    "    \n",
    "    # Serialize the weights to HDF5\n",
    "    model.save_weights(\"2.2.2.1 Settings neural networks/model\" + str(i) + \".h5\")\n",
    "    print(\"structure \" + str(i) + \" was saved successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we trained and tested our neural network for the first ticker by preventing our network to overfit using EarlyStopping as Callback we achieved pretty good results. \n",
    "\n",
    "In the next step we will reply the above steps to build our 5 independent neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run procedure 3 for all 5 tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Running model 1 /5 ----------------\n",
      "(2131, 482) (237, 482) (2131, 3) (237, 3)\n",
      "Train on 2131 samples, validate on 237 samples\n",
      "Epoch 1/200\n",
      " - 1s - loss: 0.2269 - acc: 0.5148 - val_loss: 0.1608 - val_acc: 0.6878\n",
      "Epoch 2/200\n",
      " - 0s - loss: 0.1534 - acc: 0.7316 - val_loss: 0.1557 - val_acc: 0.6878\n",
      "Epoch 3/200\n",
      " - 0s - loss: 0.1499 - acc: 0.7316 - val_loss: 0.1465 - val_acc: 0.6878\n",
      "Epoch 4/200\n",
      " - 0s - loss: 0.1403 - acc: 0.7306 - val_loss: 0.1355 - val_acc: 0.6878\n",
      "Epoch 5/200\n",
      " - 0s - loss: 0.1339 - acc: 0.7560 - val_loss: 0.1269 - val_acc: 0.7553\n",
      "Epoch 6/200\n",
      " - 0s - loss: 0.1262 - acc: 0.7668 - val_loss: 0.1183 - val_acc: 0.7595\n",
      "Epoch 7/200\n",
      " - 0s - loss: 0.1188 - acc: 0.7893 - val_loss: 0.1031 - val_acc: 0.8059\n",
      "Epoch 8/200\n",
      " - 0s - loss: 0.1124 - acc: 0.8001 - val_loss: 0.1085 - val_acc: 0.7932\n",
      "Epoch 9/200\n",
      " - 0s - loss: 0.1084 - acc: 0.8160 - val_loss: 0.0941 - val_acc: 0.8270\n",
      "Epoch 10/200\n",
      " - 0s - loss: 0.1006 - acc: 0.8198 - val_loss: 0.0930 - val_acc: 0.8101\n",
      "Epoch 11/200\n",
      " - 0s - loss: 0.0963 - acc: 0.8287 - val_loss: 0.0877 - val_acc: 0.8312\n",
      "Epoch 12/200\n",
      " - 0s - loss: 0.0936 - acc: 0.8325 - val_loss: 0.0817 - val_acc: 0.8439\n",
      "Epoch 13/200\n",
      " - 0s - loss: 0.0866 - acc: 0.8484 - val_loss: 0.0890 - val_acc: 0.8101\n",
      "Epoch 14/200\n",
      " - 0s - loss: 0.0827 - acc: 0.8602 - val_loss: 0.0768 - val_acc: 0.8228\n",
      "Epoch 15/200\n",
      " - 0s - loss: 0.0740 - acc: 0.8860 - val_loss: 0.0593 - val_acc: 0.8903\n",
      "Epoch 16/200\n",
      " - 0s - loss: 0.0702 - acc: 0.8944 - val_loss: 0.0671 - val_acc: 0.8776\n",
      "Epoch 17/200\n",
      " - 0s - loss: 0.0663 - acc: 0.8925 - val_loss: 0.0419 - val_acc: 0.9451\n",
      "Epoch 18/200\n",
      " - 0s - loss: 0.0651 - acc: 0.9005 - val_loss: 0.0532 - val_acc: 0.9114\n",
      "Epoch 19/200\n",
      " - 0s - loss: 0.0693 - acc: 0.8986 - val_loss: 0.0571 - val_acc: 0.9072\n",
      "Epoch 20/200\n",
      " - 0s - loss: 0.0623 - acc: 0.9052 - val_loss: 0.0622 - val_acc: 0.9030\n",
      "Epoch 21/200\n",
      " - 0s - loss: 0.0556 - acc: 0.9174 - val_loss: 0.0435 - val_acc: 0.9325\n",
      "Epoch 22/200\n",
      " - 0s - loss: 0.0547 - acc: 0.9127 - val_loss: 0.0360 - val_acc: 0.9451\n",
      "Epoch 23/200\n",
      " - 0s - loss: 0.0512 - acc: 0.9249 - val_loss: 0.0412 - val_acc: 0.9283\n",
      "Epoch 24/200\n",
      " - 0s - loss: 0.0509 - acc: 0.9235 - val_loss: 0.0404 - val_acc: 0.9409\n",
      "Epoch 25/200\n",
      " - 0s - loss: 0.0503 - acc: 0.9202 - val_loss: 0.0335 - val_acc: 0.9494\n",
      "Epoch 26/200\n",
      " - 0s - loss: 0.0491 - acc: 0.9249 - val_loss: 0.0460 - val_acc: 0.9325\n",
      "Epoch 27/200\n",
      " - 0s - loss: 0.0454 - acc: 0.9338 - val_loss: 0.0345 - val_acc: 0.9451\n",
      "Epoch 28/200\n",
      " - 0s - loss: 0.0446 - acc: 0.9334 - val_loss: 0.0360 - val_acc: 0.9536\n",
      "Epoch 29/200\n",
      " - 0s - loss: 0.0431 - acc: 0.9404 - val_loss: 0.0365 - val_acc: 0.9325\n",
      "Epoch 30/200\n",
      " - 0s - loss: 0.0409 - acc: 0.9423 - val_loss: 0.0502 - val_acc: 0.9072\n",
      "Epoch 00030: early stopping\n",
      "Train Score: [0.045861628150863726, 0.9169404035664007]\n",
      "Test Score: [0.05024618971385533, 0.9071729957805907]\n",
      "model 1 was saved successfully\n",
      "structure 1 was saved successfully\n",
      "------------Finished model 1 /5 ----------------\n",
      "\n",
      "------------Running model 2 /5 ----------------\n",
      "(2131, 482) (237, 482) (2131, 3) (237, 3)\n",
      "Train on 2131 samples, validate on 237 samples\n",
      "Epoch 1/200\n",
      " - 1s - loss: 0.2009 - acc: 0.7292 - val_loss: 0.0567 - val_acc: 0.9367\n",
      "Epoch 2/200\n",
      " - 0s - loss: 0.0689 - acc: 0.9413 - val_loss: 0.0472 - val_acc: 0.9367\n",
      "Epoch 3/200\n",
      " - 0s - loss: 0.0548 - acc: 0.9427 - val_loss: 0.0491 - val_acc: 0.9367\n",
      "Epoch 4/200\n",
      " - 0s - loss: 0.0516 - acc: 0.9427 - val_loss: 0.0453 - val_acc: 0.9367\n",
      "Epoch 5/200\n",
      " - 0s - loss: 0.0509 - acc: 0.9427 - val_loss: 0.0469 - val_acc: 0.9367\n",
      "Epoch 6/200\n",
      " - 0s - loss: 0.0500 - acc: 0.9427 - val_loss: 0.0462 - val_acc: 0.9367\n",
      "Epoch 7/200\n",
      " - 0s - loss: 0.0497 - acc: 0.9427 - val_loss: 0.0444 - val_acc: 0.9367\n",
      "Epoch 8/200\n",
      " - 0s - loss: 0.0491 - acc: 0.9427 - val_loss: 0.0420 - val_acc: 0.9367\n",
      "Epoch 9/200\n",
      " - 0s - loss: 0.0467 - acc: 0.9427 - val_loss: 0.0444 - val_acc: 0.9367\n",
      "Epoch 10/200\n",
      " - 0s - loss: 0.0466 - acc: 0.9427 - val_loss: 0.0435 - val_acc: 0.9367\n",
      "Epoch 11/200\n",
      " - 0s - loss: 0.0453 - acc: 0.9427 - val_loss: 0.0400 - val_acc: 0.9367\n",
      "Epoch 12/200\n",
      " - 0s - loss: 0.0438 - acc: 0.9427 - val_loss: 0.0385 - val_acc: 0.9367\n",
      "Epoch 13/200\n",
      " - 0s - loss: 0.0445 - acc: 0.9427 - val_loss: 0.0439 - val_acc: 0.9367\n",
      "Epoch 14/200\n",
      " - 0s - loss: 0.0429 - acc: 0.9427 - val_loss: 0.0391 - val_acc: 0.9367\n",
      "Epoch 15/200\n",
      " - 0s - loss: 0.0425 - acc: 0.9427 - val_loss: 0.0382 - val_acc: 0.9367\n",
      "Epoch 16/200\n",
      " - 0s - loss: 0.0413 - acc: 0.9427 - val_loss: 0.0401 - val_acc: 0.9367\n",
      "Epoch 17/200\n",
      " - 0s - loss: 0.0421 - acc: 0.9427 - val_loss: 0.0379 - val_acc: 0.9367\n",
      "Epoch 18/200\n",
      " - 0s - loss: 0.0411 - acc: 0.9427 - val_loss: 0.0361 - val_acc: 0.9367\n",
      "Epoch 19/200\n",
      " - 0s - loss: 0.0402 - acc: 0.9427 - val_loss: 0.0364 - val_acc: 0.9367\n",
      "Epoch 20/200\n",
      " - 0s - loss: 0.0400 - acc: 0.9427 - val_loss: 0.0354 - val_acc: 0.9367\n",
      "Epoch 21/200\n",
      " - 0s - loss: 0.0400 - acc: 0.9427 - val_loss: 0.0359 - val_acc: 0.9367\n",
      "Epoch 22/200\n",
      " - 0s - loss: 0.0397 - acc: 0.9427 - val_loss: 0.0354 - val_acc: 0.9367\n",
      "Epoch 23/200\n",
      " - 0s - loss: 0.0391 - acc: 0.9427 - val_loss: 0.0375 - val_acc: 0.9367\n",
      "Epoch 24/200\n",
      " - 0s - loss: 0.0415 - acc: 0.9427 - val_loss: 0.0368 - val_acc: 0.9367\n",
      "Epoch 25/200\n",
      " - 0s - loss: 0.0383 - acc: 0.9427 - val_loss: 0.0344 - val_acc: 0.9367\n",
      "Epoch 26/200\n",
      " - 0s - loss: 0.0387 - acc: 0.9427 - val_loss: 0.0341 - val_acc: 0.9367\n",
      "Epoch 27/200\n",
      " - 0s - loss: 0.0377 - acc: 0.9427 - val_loss: 0.0381 - val_acc: 0.9367\n",
      "Epoch 28/200\n",
      " - 0s - loss: 0.0370 - acc: 0.9446 - val_loss: 0.0320 - val_acc: 0.9620\n",
      "Epoch 29/200\n",
      " - 0s - loss: 0.0342 - acc: 0.9545 - val_loss: 0.0302 - val_acc: 0.9578\n",
      "Epoch 30/200\n",
      " - 0s - loss: 0.0350 - acc: 0.9540 - val_loss: 0.0250 - val_acc: 0.9662\n",
      "Epoch 31/200\n",
      " - 0s - loss: 0.0335 - acc: 0.9568 - val_loss: 0.0278 - val_acc: 0.9662\n",
      "Epoch 32/200\n",
      " - 0s - loss: 0.0328 - acc: 0.9559 - val_loss: 0.0263 - val_acc: 0.9662\n",
      "Epoch 33/200\n",
      " - 0s - loss: 0.0318 - acc: 0.9559 - val_loss: 0.0261 - val_acc: 0.9662\n",
      "Epoch 34/200\n",
      " - 0s - loss: 0.0320 - acc: 0.9554 - val_loss: 0.0242 - val_acc: 0.9662\n",
      "Epoch 35/200\n",
      " - 0s - loss: 0.0319 - acc: 0.9550 - val_loss: 0.0241 - val_acc: 0.9662\n",
      "Epoch 36/200\n",
      " - 0s - loss: 0.0312 - acc: 0.9559 - val_loss: 0.0377 - val_acc: 0.9662\n",
      "Epoch 37/200\n",
      " - 0s - loss: 0.0342 - acc: 0.9564 - val_loss: 0.0236 - val_acc: 0.9662\n",
      "Epoch 38/200\n",
      " - 0s - loss: 0.0319 - acc: 0.9564 - val_loss: 0.0237 - val_acc: 0.9662\n",
      "Epoch 39/200\n",
      " - 0s - loss: 0.0304 - acc: 0.9559 - val_loss: 0.0236 - val_acc: 0.9662\n",
      "Epoch 40/200\n",
      " - 0s - loss: 0.0301 - acc: 0.9554 - val_loss: 0.0240 - val_acc: 0.9662\n",
      "Epoch 41/200\n",
      " - 0s - loss: 0.0294 - acc: 0.9573 - val_loss: 0.0258 - val_acc: 0.9662\n",
      "Epoch 42/200\n",
      " - 0s - loss: 0.0311 - acc: 0.9573 - val_loss: 0.0243 - val_acc: 0.9662\n",
      "Epoch 00042: early stopping\n",
      "Train Score: [0.024513260606922826, 0.9582355701548568]\n",
      "Test Score: [0.024268296382189553, 0.9662447257383966]\n",
      "model 2 was saved successfully\n",
      "structure 2 was saved successfully\n",
      "------------Finished model 2 /5 ----------------\n",
      "\n",
      "------------Running model 3 /5 ----------------\n",
      "(2131, 482) (237, 482) (2131, 3) (237, 3)\n",
      "Train on 2131 samples, validate on 237 samples\n",
      "Epoch 1/200\n",
      " - 1s - loss: 0.2091 - acc: 0.8292 - val_loss: 0.1272 - val_acc: 0.8228\n",
      "Epoch 2/200\n",
      " - 0s - loss: 0.1091 - acc: 0.8522 - val_loss: 0.1104 - val_acc: 0.8228\n",
      "Epoch 3/200\n",
      " - 0s - loss: 0.1055 - acc: 0.8527 - val_loss: 0.1054 - val_acc: 0.8228\n",
      "Epoch 4/200\n",
      " - 0s - loss: 0.0996 - acc: 0.8527 - val_loss: 0.1036 - val_acc: 0.8228\n",
      "Epoch 5/200\n",
      " - 0s - loss: 0.0972 - acc: 0.8527 - val_loss: 0.1030 - val_acc: 0.8228\n",
      "Epoch 6/200\n",
      " - 0s - loss: 0.0955 - acc: 0.8527 - val_loss: 0.1016 - val_acc: 0.8228\n",
      "Epoch 7/200\n",
      " - 0s - loss: 0.0937 - acc: 0.8527 - val_loss: 0.0990 - val_acc: 0.8228\n",
      "Epoch 8/200\n",
      " - 0s - loss: 0.0900 - acc: 0.8527 - val_loss: 0.1007 - val_acc: 0.8228\n",
      "Epoch 9/200\n",
      " - 0s - loss: 0.0876 - acc: 0.8527 - val_loss: 0.0976 - val_acc: 0.8228\n",
      "Epoch 10/200\n",
      " - 0s - loss: 0.0856 - acc: 0.8527 - val_loss: 0.0970 - val_acc: 0.8228\n",
      "Epoch 11/200\n",
      " - 0s - loss: 0.0835 - acc: 0.8527 - val_loss: 0.1009 - val_acc: 0.8228\n",
      "Epoch 12/200\n",
      " - 0s - loss: 0.0849 - acc: 0.8527 - val_loss: 0.0965 - val_acc: 0.8228\n",
      "Epoch 13/200\n",
      " - 0s - loss: 0.0807 - acc: 0.8527 - val_loss: 0.0938 - val_acc: 0.8228\n",
      "Epoch 14/200\n",
      " - 0s - loss: 0.0805 - acc: 0.8527 - val_loss: 0.0924 - val_acc: 0.8228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/200\n",
      " - 0s - loss: 0.0786 - acc: 0.8531 - val_loss: 0.0924 - val_acc: 0.8228\n",
      "Epoch 16/200\n",
      " - 0s - loss: 0.0771 - acc: 0.8531 - val_loss: 0.0909 - val_acc: 0.8228\n",
      "Epoch 17/200\n",
      " - 0s - loss: 0.0764 - acc: 0.8536 - val_loss: 0.0891 - val_acc: 0.8228\n",
      "Epoch 18/200\n",
      " - 0s - loss: 0.0740 - acc: 0.8564 - val_loss: 0.0880 - val_acc: 0.8228\n",
      "Epoch 19/200\n",
      " - 0s - loss: 0.0718 - acc: 0.8559 - val_loss: 0.0866 - val_acc: 0.8312\n",
      "Epoch 20/200\n",
      " - 0s - loss: 0.0707 - acc: 0.8611 - val_loss: 0.0870 - val_acc: 0.8312\n",
      "Epoch 21/200\n",
      " - 0s - loss: 0.0709 - acc: 0.8611 - val_loss: 0.0834 - val_acc: 0.8439\n",
      "Epoch 22/200\n",
      " - 0s - loss: 0.0668 - acc: 0.8846 - val_loss: 0.0797 - val_acc: 0.8354\n",
      "Epoch 23/200\n",
      " - 0s - loss: 0.0688 - acc: 0.8864 - val_loss: 0.0791 - val_acc: 0.8565\n",
      "Epoch 24/200\n",
      " - 0s - loss: 0.0638 - acc: 0.9000 - val_loss: 0.0781 - val_acc: 0.8439\n",
      "Epoch 25/200\n",
      " - 0s - loss: 0.0637 - acc: 0.9038 - val_loss: 0.0732 - val_acc: 0.8523\n",
      "Epoch 26/200\n",
      " - 0s - loss: 0.0617 - acc: 0.9057 - val_loss: 0.0849 - val_acc: 0.8354\n",
      "Epoch 27/200\n",
      " - 0s - loss: 0.0621 - acc: 0.8982 - val_loss: 0.0855 - val_acc: 0.8143\n",
      "Epoch 28/200\n",
      " - 0s - loss: 0.0578 - acc: 0.9104 - val_loss: 0.0751 - val_acc: 0.8565\n",
      "Epoch 29/200\n",
      " - 0s - loss: 0.0483 - acc: 0.9216 - val_loss: 0.0705 - val_acc: 0.8776\n",
      "Epoch 30/200\n",
      " - 0s - loss: 0.0482 - acc: 0.9235 - val_loss: 0.0666 - val_acc: 0.8650\n",
      "Epoch 31/200\n",
      " - 0s - loss: 0.0452 - acc: 0.9249 - val_loss: 0.0635 - val_acc: 0.8776\n",
      "Epoch 32/200\n",
      " - 0s - loss: 0.0467 - acc: 0.9202 - val_loss: 0.0654 - val_acc: 0.8692\n",
      "Epoch 33/200\n",
      " - 0s - loss: 0.0422 - acc: 0.9273 - val_loss: 0.0724 - val_acc: 0.8608\n",
      "Epoch 34/200\n",
      " - 0s - loss: 0.0451 - acc: 0.9235 - val_loss: 0.0571 - val_acc: 0.8776\n",
      "Epoch 35/200\n",
      " - 0s - loss: 0.0397 - acc: 0.9376 - val_loss: 0.0624 - val_acc: 0.8819\n",
      "Epoch 36/200\n",
      " - 0s - loss: 0.0364 - acc: 0.9404 - val_loss: 0.0631 - val_acc: 0.8776\n",
      "Epoch 37/200\n",
      " - 0s - loss: 0.0415 - acc: 0.9287 - val_loss: 0.0544 - val_acc: 0.8861\n",
      "Epoch 38/200\n",
      " - 0s - loss: 0.0369 - acc: 0.9357 - val_loss: 0.0655 - val_acc: 0.8819\n",
      "Epoch 39/200\n",
      " - 0s - loss: 0.0349 - acc: 0.9381 - val_loss: 0.0642 - val_acc: 0.8776\n",
      "Epoch 40/200\n",
      " - 0s - loss: 0.0350 - acc: 0.9413 - val_loss: 0.0594 - val_acc: 0.8819\n",
      "Epoch 41/200\n",
      " - 0s - loss: 0.0346 - acc: 0.9521 - val_loss: 0.0707 - val_acc: 0.8819\n",
      "Epoch 42/200\n",
      " - 0s - loss: 0.0350 - acc: 0.9535 - val_loss: 0.0632 - val_acc: 0.8776\n",
      "Epoch 00042: early stopping\n",
      "Train Score: [0.023982051662717878, 0.9662130455185359]\n",
      "Test Score: [0.06320844373748272, 0.8776371313046806]\n",
      "model 3 was saved successfully\n",
      "structure 3 was saved successfully\n",
      "------------Finished model 3 /5 ----------------\n",
      "\n",
      "------------Running model 4 /5 ----------------\n",
      "(2131, 482) (237, 482) (2131, 3) (237, 3)\n",
      "Train on 2131 samples, validate on 237 samples\n",
      "Epoch 1/200\n",
      " - 1s - loss: 0.2825 - acc: 0.3050 - val_loss: 0.2246 - val_acc: 0.2911\n",
      "Epoch 2/200\n",
      " - 0s - loss: 0.2121 - acc: 0.4425 - val_loss: 0.1882 - val_acc: 0.6414\n",
      "Epoch 3/200\n",
      " - 0s - loss: 0.1945 - acc: 0.6279 - val_loss: 0.1830 - val_acc: 0.6414\n",
      "Epoch 4/200\n",
      " - 0s - loss: 0.1895 - acc: 0.6279 - val_loss: 0.1783 - val_acc: 0.6414\n",
      "Epoch 5/200\n",
      " - 0s - loss: 0.1849 - acc: 0.6283 - val_loss: 0.1718 - val_acc: 0.6414\n",
      "Epoch 6/200\n",
      " - 0s - loss: 0.1829 - acc: 0.6283 - val_loss: 0.1675 - val_acc: 0.6414\n",
      "Epoch 7/200\n",
      " - 0s - loss: 0.1777 - acc: 0.6283 - val_loss: 0.1642 - val_acc: 0.6414\n",
      "Epoch 8/200\n",
      " - 0s - loss: 0.1728 - acc: 0.6283 - val_loss: 0.1644 - val_acc: 0.6414\n",
      "Epoch 9/200\n",
      " - 0s - loss: 0.1736 - acc: 0.6283 - val_loss: 0.1625 - val_acc: 0.6414\n",
      "Epoch 10/200\n",
      " - 0s - loss: 0.1686 - acc: 0.6279 - val_loss: 0.1549 - val_acc: 0.6414\n",
      "Epoch 11/200\n",
      " - 0s - loss: 0.1624 - acc: 0.6405 - val_loss: 0.1452 - val_acc: 0.6920\n",
      "Epoch 12/200\n",
      " - 0s - loss: 0.1498 - acc: 0.6898 - val_loss: 0.1287 - val_acc: 0.7300\n",
      "Epoch 13/200\n",
      " - 0s - loss: 0.1339 - acc: 0.7316 - val_loss: 0.1158 - val_acc: 0.7679\n",
      "Epoch 14/200\n",
      " - 0s - loss: 0.1256 - acc: 0.7701 - val_loss: 0.1165 - val_acc: 0.7595\n",
      "Epoch 15/200\n",
      " - 0s - loss: 0.1153 - acc: 0.7860 - val_loss: 0.1008 - val_acc: 0.8059\n",
      "Epoch 16/200\n",
      " - 0s - loss: 0.1014 - acc: 0.8123 - val_loss: 0.1007 - val_acc: 0.7764\n",
      "Epoch 17/200\n",
      " - 0s - loss: 0.0966 - acc: 0.8254 - val_loss: 0.1045 - val_acc: 0.7722\n",
      "Epoch 18/200\n",
      " - 0s - loss: 0.0982 - acc: 0.8221 - val_loss: 0.0952 - val_acc: 0.7848\n",
      "Epoch 19/200\n",
      " - 0s - loss: 0.0986 - acc: 0.8189 - val_loss: 0.1085 - val_acc: 0.7722\n",
      "Epoch 20/200\n",
      " - 0s - loss: 0.0853 - acc: 0.8494 - val_loss: 0.0810 - val_acc: 0.8228\n",
      "Epoch 21/200\n",
      " - 0s - loss: 0.0803 - acc: 0.8630 - val_loss: 0.0832 - val_acc: 0.8270\n",
      "Epoch 22/200\n",
      " - 0s - loss: 0.0778 - acc: 0.8634 - val_loss: 0.0754 - val_acc: 0.8439\n",
      "Epoch 23/200\n",
      " - 0s - loss: 0.0835 - acc: 0.8531 - val_loss: 0.0908 - val_acc: 0.8101\n",
      "Epoch 24/200\n",
      " - 0s - loss: 0.0765 - acc: 0.8606 - val_loss: 0.0763 - val_acc: 0.8481\n",
      "Epoch 25/200\n",
      " - 0s - loss: 0.0724 - acc: 0.8700 - val_loss: 0.0853 - val_acc: 0.8312\n",
      "Epoch 26/200\n",
      " - 0s - loss: 0.0705 - acc: 0.8700 - val_loss: 0.0737 - val_acc: 0.8608\n",
      "Epoch 27/200\n",
      " - 0s - loss: 0.0668 - acc: 0.8846 - val_loss: 0.0796 - val_acc: 0.8439\n",
      "Epoch 28/200\n",
      " - 0s - loss: 0.0673 - acc: 0.8893 - val_loss: 0.0834 - val_acc: 0.8312\n",
      "Epoch 29/200\n",
      " - 0s - loss: 0.0632 - acc: 0.8930 - val_loss: 0.0766 - val_acc: 0.8439\n",
      "Epoch 30/200\n",
      " - 0s - loss: 0.0578 - acc: 0.9155 - val_loss: 0.0723 - val_acc: 0.8439\n",
      "Epoch 31/200\n",
      " - 0s - loss: 0.0559 - acc: 0.9108 - val_loss: 0.0764 - val_acc: 0.8650\n",
      "Epoch 32/200\n",
      " - 0s - loss: 0.0529 - acc: 0.9230 - val_loss: 0.0613 - val_acc: 0.8945\n",
      "Epoch 33/200\n",
      " - 0s - loss: 0.0504 - acc: 0.9305 - val_loss: 0.0645 - val_acc: 0.8819\n",
      "Epoch 34/200\n",
      " - 0s - loss: 0.0473 - acc: 0.9291 - val_loss: 0.0655 - val_acc: 0.8819\n",
      "Epoch 35/200\n",
      " - 0s - loss: 0.0487 - acc: 0.9291 - val_loss: 0.0499 - val_acc: 0.8987\n",
      "Epoch 36/200\n",
      " - 0s - loss: 0.0404 - acc: 0.9498 - val_loss: 0.0512 - val_acc: 0.9030\n",
      "Epoch 37/200\n",
      " - 0s - loss: 0.0377 - acc: 0.9554 - val_loss: 0.0497 - val_acc: 0.9156\n",
      "Epoch 38/200\n",
      " - 0s - loss: 0.0335 - acc: 0.9657 - val_loss: 0.0504 - val_acc: 0.9072\n",
      "Epoch 39/200\n",
      " - 0s - loss: 0.0382 - acc: 0.9535 - val_loss: 0.0588 - val_acc: 0.8903\n",
      "Epoch 40/200\n",
      " - 0s - loss: 0.0410 - acc: 0.9446 - val_loss: 0.0488 - val_acc: 0.9030\n",
      "Epoch 41/200\n",
      " - 0s - loss: 0.0400 - acc: 0.9418 - val_loss: 0.0522 - val_acc: 0.8945\n",
      "Epoch 42/200\n",
      " - 0s - loss: 0.0334 - acc: 0.9564 - val_loss: 0.0517 - val_acc: 0.9072\n",
      "Epoch 43/200\n",
      " - 0s - loss: 0.0281 - acc: 0.9690 - val_loss: 0.0374 - val_acc: 0.9325\n",
      "Epoch 44/200\n",
      " - 0s - loss: 0.0292 - acc: 0.9648 - val_loss: 0.0501 - val_acc: 0.9030\n",
      "Epoch 45/200\n",
      " - 0s - loss: 0.0307 - acc: 0.9601 - val_loss: 0.0454 - val_acc: 0.9156\n",
      "Epoch 46/200\n",
      " - 0s - loss: 0.0262 - acc: 0.9751 - val_loss: 0.0431 - val_acc: 0.9156\n",
      "Epoch 47/200\n",
      " - 0s - loss: 0.0250 - acc: 0.9700 - val_loss: 0.0371 - val_acc: 0.9451\n",
      "Epoch 48/200\n",
      " - 0s - loss: 0.0262 - acc: 0.9737 - val_loss: 0.0374 - val_acc: 0.9198\n",
      "Epoch 49/200\n",
      " - 0s - loss: 0.0233 - acc: 0.9779 - val_loss: 0.0383 - val_acc: 0.9241\n",
      "Epoch 50/200\n",
      " - 0s - loss: 0.0228 - acc: 0.9761 - val_loss: 0.0458 - val_acc: 0.9114\n",
      "Epoch 51/200\n",
      " - 0s - loss: 0.0264 - acc: 0.9690 - val_loss: 0.0438 - val_acc: 0.9198\n",
      "Epoch 52/200\n",
      " - 0s - loss: 0.0284 - acc: 0.9672 - val_loss: 0.0398 - val_acc: 0.9198\n",
      "Epoch 00052: early stopping\n",
      "Train Score: [0.010667820572538423, 0.9863913657518688]\n",
      "Test Score: [0.03977362418602288, 0.9198312241316847]\n",
      "model 4 was saved successfully\n",
      "structure 4 was saved successfully\n",
      "------------Finished model 4 /5 ----------------\n",
      "\n",
      "------------Running model 5 /5 ----------------\n",
      "(1137, 482) (127, 482) (1137, 3) (127, 3)\n",
      "Train on 1137 samples, validate on 127 samples\n",
      "Epoch 1/200\n",
      " - 1s - loss: 0.2958 - acc: 0.7300 - val_loss: 0.1718 - val_acc: 0.8031\n",
      "Epoch 2/200\n",
      " - 0s - loss: 0.1582 - acc: 0.7581 - val_loss: 0.1194 - val_acc: 0.8031\n",
      "Epoch 3/200\n",
      " - 0s - loss: 0.1441 - acc: 0.7581 - val_loss: 0.1096 - val_acc: 0.8031\n",
      "Epoch 4/200\n",
      " - 0s - loss: 0.1388 - acc: 0.7581 - val_loss: 0.1107 - val_acc: 0.8031\n",
      "Epoch 5/200\n",
      " - 0s - loss: 0.1388 - acc: 0.7581 - val_loss: 0.1091 - val_acc: 0.8031\n",
      "Epoch 6/200\n",
      " - 0s - loss: 0.1370 - acc: 0.7581 - val_loss: 0.1071 - val_acc: 0.8031\n",
      "Epoch 7/200\n",
      " - 0s - loss: 0.1350 - acc: 0.7581 - val_loss: 0.1059 - val_acc: 0.8031\n",
      "Epoch 8/200\n",
      " - 0s - loss: 0.1338 - acc: 0.7581 - val_loss: 0.1063 - val_acc: 0.8031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/200\n",
      " - 0s - loss: 0.1344 - acc: 0.7581 - val_loss: 0.1064 - val_acc: 0.8031\n",
      "Epoch 10/200\n",
      " - 0s - loss: 0.1340 - acc: 0.7581 - val_loss: 0.1042 - val_acc: 0.8031\n",
      "Epoch 11/200\n",
      " - 0s - loss: 0.1314 - acc: 0.7581 - val_loss: 0.1053 - val_acc: 0.8031\n",
      "Epoch 12/200\n",
      " - 0s - loss: 0.1320 - acc: 0.7581 - val_loss: 0.1085 - val_acc: 0.8031\n",
      "Epoch 13/200\n",
      " - 0s - loss: 0.1303 - acc: 0.7581 - val_loss: 0.1026 - val_acc: 0.8031\n",
      "Epoch 14/200\n",
      " - 0s - loss: 0.1264 - acc: 0.7581 - val_loss: 0.0984 - val_acc: 0.8031\n",
      "Epoch 15/200\n",
      " - 0s - loss: 0.1274 - acc: 0.7581 - val_loss: 0.0964 - val_acc: 0.8031\n",
      "Epoch 16/200\n",
      " - 0s - loss: 0.1212 - acc: 0.7581 - val_loss: 0.0945 - val_acc: 0.8031\n",
      "Epoch 17/200\n",
      " - 0s - loss: 0.1185 - acc: 0.7581 - val_loss: 0.0957 - val_acc: 0.8031\n",
      "Epoch 18/200\n",
      " - 0s - loss: 0.1170 - acc: 0.7581 - val_loss: 0.0959 - val_acc: 0.8031\n",
      "Epoch 19/200\n",
      " - 0s - loss: 0.1161 - acc: 0.7581 - val_loss: 0.0929 - val_acc: 0.8031\n",
      "Epoch 20/200\n",
      " - 0s - loss: 0.1139 - acc: 0.7581 - val_loss: 0.0940 - val_acc: 0.8031\n",
      "Epoch 21/200\n",
      " - 0s - loss: 0.1119 - acc: 0.7581 - val_loss: 0.0938 - val_acc: 0.8031\n",
      "Epoch 22/200\n",
      " - 0s - loss: 0.1133 - acc: 0.7581 - val_loss: 0.0905 - val_acc: 0.8031\n",
      "Epoch 23/200\n",
      " - 0s - loss: 0.1099 - acc: 0.7581 - val_loss: 0.0910 - val_acc: 0.8031\n",
      "Epoch 24/200\n",
      " - 0s - loss: 0.1076 - acc: 0.7581 - val_loss: 0.0897 - val_acc: 0.8031\n",
      "Epoch 25/200\n",
      " - 0s - loss: 0.1068 - acc: 0.7599 - val_loss: 0.0902 - val_acc: 0.8110\n",
      "Epoch 26/200\n",
      " - 0s - loss: 0.1058 - acc: 0.7722 - val_loss: 0.0915 - val_acc: 0.8110\n",
      "Epoch 27/200\n",
      " - 0s - loss: 0.1054 - acc: 0.7722 - val_loss: 0.0893 - val_acc: 0.8110\n",
      "Epoch 28/200\n",
      " - 0s - loss: 0.1041 - acc: 0.7792 - val_loss: 0.0908 - val_acc: 0.8110\n",
      "Epoch 29/200\n",
      " - 0s - loss: 0.1036 - acc: 0.7792 - val_loss: 0.0923 - val_acc: 0.8031\n",
      "Epoch 30/200\n",
      " - 0s - loss: 0.1042 - acc: 0.7801 - val_loss: 0.0880 - val_acc: 0.8110\n",
      "Epoch 31/200\n",
      " - 0s - loss: 0.1036 - acc: 0.7854 - val_loss: 0.0923 - val_acc: 0.8110\n",
      "Epoch 32/200\n",
      " - 0s - loss: 0.1018 - acc: 0.7836 - val_loss: 0.0861 - val_acc: 0.8110\n",
      "Epoch 33/200\n",
      " - 0s - loss: 0.1010 - acc: 0.7836 - val_loss: 0.0850 - val_acc: 0.8110\n",
      "Epoch 34/200\n",
      " - 0s - loss: 0.1003 - acc: 0.7819 - val_loss: 0.0830 - val_acc: 0.8110\n",
      "Epoch 35/200\n",
      " - 0s - loss: 0.0979 - acc: 0.7828 - val_loss: 0.0854 - val_acc: 0.8189\n",
      "Epoch 36/200\n",
      " - 0s - loss: 0.0980 - acc: 0.7863 - val_loss: 0.0869 - val_acc: 0.8189\n",
      "Epoch 37/200\n",
      " - 0s - loss: 0.0960 - acc: 0.7854 - val_loss: 0.0797 - val_acc: 0.8110\n",
      "Epoch 38/200\n",
      " - 0s - loss: 0.0926 - acc: 0.8153 - val_loss: 0.0820 - val_acc: 0.8819\n",
      "Epoch 39/200\n",
      " - 0s - loss: 0.0903 - acc: 0.8804 - val_loss: 0.0727 - val_acc: 0.9055\n",
      "Epoch 40/200\n",
      " - 0s - loss: 0.0715 - acc: 0.9191 - val_loss: 0.0599 - val_acc: 0.9213\n",
      "Epoch 41/200\n",
      " - 0s - loss: 0.0576 - acc: 0.9252 - val_loss: 0.0587 - val_acc: 0.9055\n",
      "Epoch 42/200\n",
      " - 0s - loss: 0.0545 - acc: 0.9208 - val_loss: 0.0599 - val_acc: 0.8819\n",
      "Epoch 43/200\n",
      " - 0s - loss: 0.0471 - acc: 0.9305 - val_loss: 0.0572 - val_acc: 0.8976\n",
      "Epoch 44/200\n",
      " - 0s - loss: 0.0447 - acc: 0.9393 - val_loss: 0.0462 - val_acc: 0.9134\n",
      "Epoch 45/200\n",
      " - 0s - loss: 0.0454 - acc: 0.9402 - val_loss: 0.0421 - val_acc: 0.9291\n",
      "Epoch 46/200\n",
      " - 0s - loss: 0.0361 - acc: 0.9560 - val_loss: 0.0393 - val_acc: 0.9370\n",
      "Epoch 47/200\n",
      " - 0s - loss: 0.0364 - acc: 0.9481 - val_loss: 0.0993 - val_acc: 0.8189\n",
      "Epoch 48/200\n",
      " - 0s - loss: 0.0532 - acc: 0.9244 - val_loss: 0.0429 - val_acc: 0.9134\n",
      "Epoch 49/200\n",
      " - 0s - loss: 0.0401 - acc: 0.9446 - val_loss: 0.0454 - val_acc: 0.9291\n",
      "Epoch 50/200\n",
      " - 0s - loss: 0.0368 - acc: 0.9534 - val_loss: 0.0414 - val_acc: 0.9213\n",
      "Epoch 51/200\n",
      " - 0s - loss: 0.0331 - acc: 0.9604 - val_loss: 0.0366 - val_acc: 0.9528\n",
      "Epoch 52/200\n",
      " - 0s - loss: 0.0335 - acc: 0.9551 - val_loss: 0.0395 - val_acc: 0.9213\n",
      "Epoch 53/200\n",
      " - 0s - loss: 0.0307 - acc: 0.9587 - val_loss: 0.0379 - val_acc: 0.9291\n",
      "Epoch 54/200\n",
      " - 0s - loss: 0.0304 - acc: 0.9569 - val_loss: 0.0349 - val_acc: 0.9370\n",
      "Epoch 55/200\n",
      " - 0s - loss: 0.0395 - acc: 0.9455 - val_loss: 0.0434 - val_acc: 0.9134\n",
      "Epoch 56/200\n",
      " - 0s - loss: 0.0316 - acc: 0.9622 - val_loss: 0.0480 - val_acc: 0.8976\n",
      "Epoch 57/200\n",
      " - 0s - loss: 0.0324 - acc: 0.9595 - val_loss: 0.0363 - val_acc: 0.9370\n",
      "Epoch 58/200\n",
      " - 0s - loss: 0.0269 - acc: 0.9657 - val_loss: 0.0346 - val_acc: 0.9213\n",
      "Epoch 59/200\n",
      " - 0s - loss: 0.0300 - acc: 0.9622 - val_loss: 0.0370 - val_acc: 0.9449\n",
      "Epoch 60/200\n",
      " - 0s - loss: 0.0278 - acc: 0.9683 - val_loss: 0.0367 - val_acc: 0.9291\n",
      "Epoch 61/200\n",
      " - 0s - loss: 0.0265 - acc: 0.9657 - val_loss: 0.0367 - val_acc: 0.9213\n",
      "Epoch 62/200\n",
      " - 0s - loss: 0.0264 - acc: 0.9657 - val_loss: 0.0373 - val_acc: 0.9134\n",
      "Epoch 63/200\n",
      " - 0s - loss: 0.0243 - acc: 0.9727 - val_loss: 0.0324 - val_acc: 0.9528\n",
      "Epoch 64/200\n",
      " - 0s - loss: 0.0244 - acc: 0.9675 - val_loss: 0.0454 - val_acc: 0.9134\n",
      "Epoch 65/200\n",
      " - 0s - loss: 0.0279 - acc: 0.9666 - val_loss: 0.0976 - val_acc: 0.8110\n",
      "Epoch 66/200\n",
      " - 0s - loss: 0.0451 - acc: 0.9296 - val_loss: 0.0353 - val_acc: 0.9370\n",
      "Epoch 67/200\n",
      " - 0s - loss: 0.0292 - acc: 0.9692 - val_loss: 0.0357 - val_acc: 0.9449\n",
      "Epoch 68/200\n",
      " - 0s - loss: 0.0231 - acc: 0.9807 - val_loss: 0.0393 - val_acc: 0.9291\n",
      "Epoch 00068: early stopping\n",
      "Train Score: [0.026400110934162307, 0.9542656114149639]\n",
      "Test Score: [0.039323208926934895, 0.9291338512277979]\n",
      "model 5 was saved successfully\n",
      "structure 5 was saved successfully\n",
      "------------Finished model 5 /5 ----------------\n",
      "\n",
      "5 Neural Models were trained\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,6):\n",
    "\n",
    "    print(\"------------Running model \" + str(i) + \" /5 ----------------\")\n",
    "    \n",
    "    # 1. Load the data \n",
    "    df = pd.read_csv('Prepared data/Training_set_NN' + str(i) + '.csv')\n",
    "    df = df.drop('Unnamed: 0',axis=1)\n",
    "    \n",
    "    # 2. Separate the data into X and y\n",
    "    X = df.drop([\"Y1\",\"Y2\",\"Y3\",\"Date\"],axis=1).values  \n",
    "    y = df.loc[:, df.columns.intersection(['Y1','Y2','Y3'])].values\n",
    "\n",
    "    # 3. Split the data into training and testing set\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.1)\n",
    "    print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)\n",
    "    \n",
    "    # 3.1 Safetycheck if dataframe is empty\n",
    "    if len(X_train) == 0 or len(y_train) == 0 or len(X_test) == 0 or len(y_test) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # 4. Test and train neural network using earlystopping\n",
    "    earlystopping = callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, verbose=4)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(500, input_dim=482, activation=\"relu\", kernel_initializer=\"normal\", \n",
    "                    kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(100, activation=\"relu\", kernel_initializer=\"normal\", \n",
    "                    kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(50, activation=\"relu\", kernel_initializer=\"normal\", \n",
    "                    kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(10, activation=\"relu\", kernel_initializer=\"normal\", \n",
    "                    kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(3, activation=\"relu\", kernel_initializer=\"normal\"))\n",
    "\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer=\"Adam\", metrics=['accuracy'])\n",
    "                \n",
    "    Neural_model = model.fit(X_train, y_train, epochs=200, batch_size=100, verbose=2,\n",
    "                         validation_data=(X_test, y_test), callbacks = [earlystopping])\n",
    "    \n",
    "    \n",
    "    # 5. print evaluation metrics\n",
    "    trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
    "    print('Train Score: ' + str(trainScore))\n",
    "    testScore = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test Score: ' + str(testScore))\n",
    "    \n",
    "    \n",
    "    # 6. Save model weights\n",
    "    # Serialize the model to json\n",
    "    model_json = model.to_json()\n",
    "    json_file = open(\"2.2.2.1 Settings neural networks/model\" + str(i) + \".json\", 'w')\n",
    "    json_file.write(model_json)\n",
    "    json_file.close()\n",
    "    print(\"model \" + str(i) + \" was saved successfully\")\n",
    "    \n",
    "    # Serialize the weights to HDF5\n",
    "    model.save_weights(\"2.2.2.1 Settings neural networks/model\" + str(i) + \".h5\")\n",
    "    print(\"structure \" + str(i) + \" was saved successfully\")\n",
    "    \n",
    "    print(\"------------Finished model \" + str(i) + \" /5 ----------------\\n\")\n",
    "print(str(i) + \" Neural Models were trained\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the output field above each model have run through a different number of epochs before the training was stopped. You can also see, that all models have a test accuracy higher 87%. The accuracy varies between 87,7% and  96,6%.\n",
    "\n",
    "After all 5 neural networks are trained and saved accordingly, we can now proceed to the prediction part of our project.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
